{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ae1d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a477112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from imutils import paths\n",
    "# from tqdm import tqdm\n",
    "# # import pandas as pd \n",
    "# # import numpy as np\n",
    "# import shutil\n",
    "# # import cv2\n",
    "# # import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02582de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "\n",
    "MAX_SEQ_LENGTH = 40\n",
    "NUM_FEATURES = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd20e086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"train.csv\")\n",
    "# test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# print(f\"Total videos for training: {len(train_df)}\")\n",
    "# print(f\"Total videos for testing: {len(test_df)}\")\n",
    "\n",
    "# train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f879426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The following two methods are taken from this tutorial:\n",
    "# # https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
    "\n",
    "\n",
    "# def crop_center_square(frame):\n",
    "#     y, x = frame.shape[0:2]\n",
    "#     min_dim = min(y, x)\n",
    "#     start_x = (x // 2) - (min_dim // 2)\n",
    "#     start_y = (y // 2) - (min_dim // 2)\n",
    "#     return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "# def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "#     cap = cv2.VideoCapture(path)\n",
    "#     frames = []\n",
    "#     try:\n",
    "#         while True:\n",
    "#             ret, frame = cap.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "#             frame = crop_center_square(frame)\n",
    "#             frame = cv2.resize(frame, resize)\n",
    "#             frame = frame[:, :, [2, 1, 0]]\n",
    "#             frames.append(frame)\n",
    "\n",
    "#             if len(frames) == max_frames:\n",
    "#                 break\n",
    "#     finally:\n",
    "#         cap.release()\n",
    "#     return np.array(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7df79be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def build_feature_extractor():\n",
    "#     feature_extractor = keras.applications.DenseNet121(\n",
    "#         weights=\"imagenet\",\n",
    "#         include_top=False,\n",
    "#         pooling=\"avg\",\n",
    "#         input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "#     )\n",
    "#     preprocess_input = keras.applications.densenet.preprocess_input\n",
    "\n",
    "#     inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "#     preprocessed = preprocess_input(inputs)\n",
    "\n",
    "#     outputs = feature_extractor(preprocessed)\n",
    "#     return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "# feature_extractor = build_feature_extractor()\n",
    "\n",
    "# # Label preprocessing with StringLookup.\n",
    "# label_processor = keras.layers.StringLookup(\n",
    "#     num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]), mask_token=None\n",
    "# )\n",
    "# print(label_processor.get_vocabulary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d410f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_extractor.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e011e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos for training: 9537\n",
      "Total videos for testing: 3783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 09:48:19.041057: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-04 09:48:19.564078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10381 MB memory:  -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress', 'Biking', 'Billiards', 'BlowDryHair', 'BlowingCandles', 'BodyWeightSquats', 'Bowling', 'BoxingPunchingBag', 'BoxingSpeedBag', 'BreastStroke', 'BrushingTeeth', 'CleanAndJerk', 'CliffDiving', 'CricketBowling', 'CricketShot', 'CuttingInKitchen', 'Diving', 'Drumming', 'Fencing', 'FieldHockeyPenalty', 'FloorGymnastics', 'FrisbeeCatch', 'FrontCrawl', 'GolfSwing', 'Haircut', 'HammerThrow', 'Hammering', 'HandstandPushups', 'HandstandWalking', 'HeadMassage', 'HighJump', 'HorseRace', 'HorseRiding', 'HulaHoop', 'IceDancing', 'JavelinThrow', 'JugglingBalls', 'JumpRope', 'JumpingJack', 'Kayaking', 'Knitting', 'LongJump', 'Lunges', 'MilitaryParade', 'Mixing', 'MoppingFloor', 'Nunchucks', 'ParallelBars', 'PizzaTossing', 'PlayingCello', 'PlayingDaf', 'PlayingDhol', 'PlayingFlute', 'PlayingGuitar', 'PlayingPiano', 'PlayingSitar', 'PlayingTabla', 'PlayingViolin', 'PoleVault', 'PommelHorse', 'PullUps', 'Punch', 'PushUps', 'Rafting', 'RockClimbingIndoor', 'RopeClimbing', 'Rowing', 'SalsaSpin', 'ShavingBeard', 'Shotput', 'SkateBoarding', 'Skiing', 'Skijet', 'SkyDiving', 'SoccerJuggling', 'SoccerPenalty', 'StillRings', 'SumoWrestling', 'Surfing', 'Swing', 'TableTennisShot', 'TaiChi', 'TennisSwing', 'ThrowDiscus', 'TrampolineJumping', 'Typing', 'UnevenBars', 'VolleyballSpiking', 'WalkingWithDog', 'WallPushups', 'WritingOnBoard', 'YoYo']\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")\n",
    "\n",
    "center_crop_layer = keras.layers.CenterCrop(IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "\n",
    "def crop_center(frame):\n",
    "    cropped = center_crop_layer(frame[None, ...])\n",
    "    cropped = cropped.numpy().squeeze()\n",
    "    return cropped\n",
    "\n",
    "\n",
    "# Following method is modified from this tutorial:\n",
    "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
    "def load_video(path, max_frames=0):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center(frame)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "\n",
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.DenseNet121(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.densenet.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()\n",
    "\n",
    "\n",
    "# Label preprocessing with StringLookup.\n",
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]), mask_token=None\n",
    ")\n",
    "print(label_processor.get_vocabulary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b17d8808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_name\"].values.tolist()\n",
    "    labels = df[\"tag\"].values\n",
    "    labels = label_processor(labels[..., None]).numpy()\n",
    "\n",
    "    # `frame_features` are what we will feed to our sequence model.\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in tqdm(enumerate(video_paths)):\n",
    "#         print(idx)\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "\n",
    "        # Pad shorter videos.\n",
    "        if len(frames) < MAX_SEQ_LENGTH:\n",
    "            diff = MAX_SEQ_LENGTH - len(frames)\n",
    "            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
    "            frames = np.concatenate(frames, padding)\n",
    "\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        # Initialize placeholder to store the features of the current video.\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                if np.mean(batch[j, :]) > 0.0:\n",
    "                    temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                        batch[None, j, :]\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    temp_frame_features[i, j, :] = 0.0\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "\n",
    "    return frame_features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "518120d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]2022-07-04 09:52:24.042037: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n",
      "2022-07-04 09:52:24.250621: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-07-04 09:52:24.251341: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-07-04 09:52:24.251398: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2022-07-04 09:52:24.251881: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-07-04 09:52:24.251976: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "3976it [3:25:32,  3.10s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data, train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_all_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m test_data, test_labels \u001b[38;5;241m=\u001b[39m prepare_all_videos(test_df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrame features in train set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mprepare_all_videos\u001b[0;34m(df, root_dir)\u001b[0m\n\u001b[1;32m     20\u001b[0m     diff \u001b[38;5;241m=\u001b[39m MAX_SEQ_LENGTH \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(frames)\n\u001b[1;32m     21\u001b[0m     padding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((diff, IMG_SIZE, IMG_SIZE, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m---> 22\u001b[0m     frames \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m frames \u001b[38;5;241m=\u001b[39m frames[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Initialize placeholder to store the features of the current video.\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
    "test_data, test_labels = prepare_all_videos(test_df, \"test\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638eadc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a3ecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('ResNet101_CNNRNN_train_40.pickle', 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "\n",
    "with open('ResNet101_CNNRNN_test_40.pickle', 'wb') as f:\n",
    "    pickle.dump(test_data, f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08d8f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import pickle\n",
    "\n",
    "with open('Inception_CNNRNN_40.pickle', 'rb') as f:\n",
    "     d_train_data = pickle.load(f)\n",
    "with open('Inception_CNNRNN_test_40.pickle', 'rb') as f:\n",
    "     d_test_data = pickle.load(f)\n",
    "        \n",
    "# d_train_data = np.load('densenet_train_data.npy')\n",
    "# d_test_data = np.load('densenet_test_data.npy')\n",
    "d_train_labels = np.load('CNNRNN_train_labels.npy')\n",
    "d_test_labels = np.load('CNNRNN_test_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4f9a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"])\n",
    ")\n",
    "# print(label_processor.get_vocabulary())\n",
    "\n",
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    # Refer to the following tutorial to understand the significance of using `mask`:\n",
    "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
    "    x = keras.layers.LSTM(2048, return_sequences=False,dropout=0.5)(#16, 512\n",
    "        frame_features_input, mask=mask_input\n",
    "    )    \n",
    "#     x = keras.layers.Bidirectional(keras.layers.LSTM(2048, return_sequences=False,dropout=0.5),merge_mode='concat')(#16, 512\n",
    "#         frame_features_input, mask=mask_input\n",
    "#     )\n",
    "#     x = keras.layers.LSTM(2048)(x)#8, 256\n",
    "#     x = keras.layers.Dropout(0.1)(x)\n",
    "    x = keras.layers.Dense(1024)(x)#8, 256\n",
    "    x = keras.layers.GaussianNoise(0.4)(x)\n",
    "    x = keras.layers.LeakyReLU(0.1)(x)    \n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "# Utility for running experiments.\n",
    "def d_run_experiment():\n",
    "    filepath = \"/tmp/video_classifier\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    print(seq_model.summary())\n",
    "    \n",
    "    history = seq_model.fit(\n",
    "        [d_train_data[0], d_train_data[1]],\n",
    "        d_train_labels,\n",
    "#         validation_split=0.2,\n",
    "        validation_data=([d_test_data[0], d_test_data[1]],\n",
    "        d_test_labels),\n",
    "        epochs=20,\n",
    "#         epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([d_test_data[0], d_test_data[1]], d_test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "\n",
    "d_run_experiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
