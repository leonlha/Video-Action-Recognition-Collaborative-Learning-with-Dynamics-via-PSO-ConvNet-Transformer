{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef3b7bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/datastorage/Phong/ucf101_v2/set1\n"
     ]
    }
   ],
   "source": [
    "cd /media/datastorage/Phong/ucf101_v2/set1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfee7320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 40\r\n",
      "drwxrwxr-x   2 bribeiro bribeiro 4096 set 28 16:35 \u001b[0m\u001b[01;34mcheckpoints\u001b[0m/\r\n",
      "drwxrwxr-x 103 bribeiro bribeiro 4096 set  5 16:22 \u001b[01;34mtest\u001b[0m/\r\n",
      "drwxrwxr-x 103 bribeiro bribeiro 4096 set  5 16:22 \u001b[01;34mtrain\u001b[0m/\r\n",
      "-rw-rw-r--   1 bribeiro bribeiro  463 jan 29  2018 ucf101_1.csv\r\n",
      "-rw-rw-r--   1 bribeiro bribeiro  643 fev  3  2018 ucf101_1_resnet152_set1.csv\r\n",
      "-rw-rw-r--   1 bribeiro bribeiro  727 jan 29  2018 ucf101_1_resnet152_set1_frame2.csv\r\n",
      "-rw-rw-r--   1 bribeiro bribeiro  797 fev  2  2018 ucf101_TransformerDenseNet201_set1.csv\r\n",
      "-rw-rw-r--   1 bribeiro bribeiro  834 fev  4  2018 ucf101_TransformerDenseNet201_set1_D2.csv\r\n",
      "-rw-rw-r--   1 bribeiro bribeiro  675 set 28 16:35 ucf101_TransformerResNet152_set1_D1.csv\r\n",
      "-rw-rw-r--   1 bribeiro bribeiro  811 set 25 23:50 ucf101_TransformerResNet152_set1_D2.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fd9704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08010f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91296015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "d = {'id': [1, 2, 3, 4], 'pbest_value': [0, 0, 0, 0], 'pbest_file':['ucf101_1_ResNet152Transformer_set1_D1_best.hdf5',\n",
    "                                                        'ucf101_2_ResNet152Transformer_set1_D1_best.hdf5',\n",
    "                                                        'ucf101_3_ResNet152Transformer_set1_D1_best.hdf5',\n",
    "                                                        'ucf101_4_ResNet152Transformer_set1_D1_best.hdf5'], \n",
    "                         'c_value': [0, 0, 0, 0], 'c_file':['ucf101_1_ResNet152Transformer_set1_D1.hdf5',\n",
    "                                                             'ucf101_2_ResNet152Transformer_set1_D1.hdf5',\n",
    "                                                             'ucf101_3_ResNet152Transformer_set1_D1.hdf5',\n",
    "                                                             'ucf101_4_ResNet152Transformer_set1_D1.hdf5'], \n",
    "                         'pre_value': [0, 0, 0, 0], 'pre_file':['ucf101_1_ResNet152Transformer_set1_D1_pre.hdf5',\n",
    "                                                             'ucf101_2_ResNet152Transformer_set1_D1_pre.hdf5',\n",
    "                                                             'ucf101_3_ResNet152Transformer_set1_D1_pre.hdf5',\n",
    "                                                             'ucf101_4_ResNet152Transformer_set1_D1_pre.hdf5'],\n",
    "                         'training_flag':[0, 0, 0, 0]\n",
    "    }\n",
    "df = pandas.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea553d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a97babd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first instance only\n",
    "df.to_csv(os.path.join('ucf101_TransformerResNet152_set1_D1.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9b7050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = os.path.join('ucf101_TransformerResNet152_set1_D1.csv')\n",
    "\n",
    "def synch_read_data(data_file=''):\n",
    "    while(True):\n",
    "        try:\n",
    "            df = pandas.read_csv(data_file, index_col=0)  \n",
    "            break                     \n",
    "        except:\n",
    "            #waiting for 10s\n",
    "            print(\"\\n\")\n",
    "            for i in range(10,0,-1):\n",
    "                print(\"re-read the file ....%2d\" %i, end=\"\\r\", flush=True)\n",
    "                time.sleep(1) \n",
    "    return df  \n",
    "\n",
    "def synch_write_data(df,data_file=''):\n",
    "    while(True):\n",
    "        try:\n",
    "            df.to_csv(data_file)  \n",
    "            break                     \n",
    "        except:\n",
    "            #waiting for 10s\n",
    "            print(\"\\n\")\n",
    "            for i in range(10,0,-1):\n",
    "                print(\"re-read the file ....%2d\" %i, end=\"\\r\", flush=True)\n",
    "                time.sleep(1) \n",
    "    return df  \n",
    "\n",
    "def get_pbest_loc(row=0):\n",
    "    df = synch_read_data(data_file)\n",
    "    row=df.loc[row]\n",
    "    pbest_value = row[1]\n",
    "    file_name = row[2]\n",
    "    return pbest_value, file_name\n",
    "\n",
    "def set_pbest_loc(row, pbest_value):\n",
    "    df = synch_read_data(data_file)\n",
    "    df.loc[row, 'pbest_value'] = pbest_value\n",
    "    synch_write_data(df,data_file)\n",
    "    \n",
    "def get_c_loc(row=0):\n",
    "    df = synch_read_data(data_file)\n",
    "    row=df.loc[row]\n",
    "    c_value = row[3]\n",
    "    file_name = row[4]\n",
    "    return c_value, file_name\n",
    "\n",
    "def set_c_loc(row, c_value):\n",
    "    df = synch_read_data(data_file)\n",
    "    df.loc[row, 'c_value'] = c_value\n",
    "    synch_write_data(df,data_file)   \n",
    "\n",
    "#    \n",
    "def get_pre_loc(row=0):\n",
    "    df = synch_read_data(data_file)\n",
    "    row=df.loc[row]\n",
    "    pre_value = row[5]\n",
    "    file_name = row[6]\n",
    "    return pre_value, file_name\n",
    "\n",
    "def set_pre_loc(row, pre_value):\n",
    "    df = synch_read_data(data_file)\n",
    "    df.loc[row, 'pre_value'] = pre_value\n",
    "    synch_write_data(df,data_file)\n",
    "    \n",
    "#training flag\n",
    "def get_training_flag(row=0):\n",
    "    df = synch_read_data(data_file)\n",
    "    row=df.loc[row]\n",
    "    training_flag = row[7]\n",
    "    return training_flag\n",
    "\n",
    "def set_training_flag(row, training_status):\n",
    "    df = synch_read_data(data_file)\n",
    "    df.loc[row, 'training_flag'] = training_status\n",
    "    synch_write_data(df,data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "518e1c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_distance(w1, w2):\n",
    "    sqr_distance = 0\n",
    "    \n",
    "    w_np_1 = np.array(w1)\n",
    "    w_fl_1 = w_np_1.flatten()\n",
    "    w_np_2 = np.array(w2)\n",
    "    w_fl_2 = w_np_2.flatten()\n",
    "    \n",
    "    for i in range(len(w_np_1)):\n",
    "        x1_fl = w_fl_1[i].flatten()\n",
    "        x2_fl = w_fl_2[i].flatten()\n",
    "\n",
    "        tmp_dis = 0 \n",
    "        for j in range(len(x1_fl)):\n",
    "            tmp_dis = tmp_dis + (x1_fl[j]-x2_fl[j])**2\n",
    "\n",
    "    #     print(tmp_dis)\n",
    "        sqr_distance = sqr_distance + tmp_dis\n",
    "\n",
    "    return sqr_distance**(1/2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f92ae7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "#Stop training on val_acc\n",
    "class EarlyStoppingByAccVal(Callback):\n",
    "    def __init__(self, monitor='val_acc', value=0.00001, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "\n",
    "        if current >= self.value:\n",
    "            if self.verbose > 0:\n",
    "                print(\"Epoch %05d: early stopping\" % epoch)\n",
    "            self.model.stop_training = True\n",
    "\n",
    "#Save large model using pickle formate instead of h5            \n",
    "class SaveCheckPoint(Callback):\n",
    "    def __init__(self, model, dest_folder):\n",
    "        super(Callback, self).__init__()\n",
    "        self.model = model\n",
    "        self.dest_folder = dest_folder\n",
    "        \n",
    "        #initiate\n",
    "        self.best_val_acc = 0\n",
    "        self.best_val_loss = sys.maxsize #get max value\n",
    "          \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_acc = logs['val_acc']\n",
    "        val_loss = logs['val_loss']\n",
    "\n",
    "        if val_acc > self.best_val_acc:\n",
    "            self.best_val_acc = val_acc\n",
    "            \n",
    "            # Save weights in pickle format instead of h5\n",
    "            print('\\nSaving val_acc %f at %s' %(self.best_val_acc, self.dest_folder))\n",
    "            weigh= self.model.get_weights()\n",
    "\n",
    "            #now, use pickle to save your model weights, instead of .h5\n",
    "            #for heavy model architectures, .h5 file is unsupported.\n",
    "            fpkl= open(self.dest_folder, 'wb') #Python 3\n",
    "            pickle.dump(weigh, fpkl, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "            fpkl.close()\n",
    "            \n",
    "#             model.save('tmp.h5')\n",
    "        elif val_acc == self.best_val_acc:\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss=val_loss\n",
    "                \n",
    "                # Save weights in pickle format instead of h5\n",
    "                print('\\nSaving val_acc %f at %s' %(self.best_val_acc, self.dest_folder))\n",
    "                weigh= self.model.get_weights()\n",
    "\n",
    "                #now, use pickle to save your model weights, instead of .h5\n",
    "                #for heavy model architectures, .h5 file is unsupported.\n",
    "                fpkl= open(self.dest_folder, 'wb') #Python 3\n",
    "                pickle.dump(weigh, fpkl, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "                fpkl.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ababf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import paths\n",
    "from tqdm import tqdm\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import shutil\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f54a147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open the .txt file which have names of training videos\n",
    "# f = open(\"ucfTrainTestlist/trainlist01.txt\", \"r\")\n",
    "# temp = f.read()\n",
    "# videos = temp.split('\\n')\n",
    "\n",
    "# # Create a dataframe having video names\n",
    "# train = pd.DataFrame()\n",
    "# train['video_name'] = videos\n",
    "# train = train[:-1]\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f4a77f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a0bf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open the .txt file which have names of test videos\n",
    "# with open(\"ucfTrainTestlist/testlist01.txt\", \"r\") as f:\n",
    "#     temp = f.read()\n",
    "# videos = temp.split(\"\\n\")\n",
    "\n",
    "# # Create a dataframe having video names\n",
    "# test = pd.DataFrame()\n",
    "# test[\"video_name\"] = videos\n",
    "# test = test[:-1]\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7629a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_tag(video_path):\n",
    "#     return video_path.split(\"/\")[0]\n",
    "\n",
    "# def separate_video_name(video_name):\n",
    "#     return video_name.split(\"/\")[1]\n",
    "\n",
    "# def rectify_video_name(video_name):\n",
    "#     return video_name.split(\" \")[0]\n",
    "\n",
    "# # def move_videos(df, output_dir):\n",
    "# #     if not os.path.exists(output_dir):\n",
    "# #         os.mkdir(output_dir)\n",
    "# #     for i in tqdm(range(df.shape[0])):\n",
    "# #         videoFile = df['video_name'][i].split(\"/\")[-1]\n",
    "# #         videoPath = os.path.join(\"data\", videoFile)\n",
    "# #         shutil.copy2(videoPath, output_dir)\n",
    "# #     print()\n",
    "# #     print(f\"Total videos: {len(os.listdir(output_dir))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1286dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[\"tag\"] = train[\"video_name\"].apply(extract_tag)\n",
    "# train[\"video_name\"] = train[\"video_name\"].apply(separate_video_name)\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e0e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[\"video_name\"] = train[\"video_name\"].apply(rectify_video_name)\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4449d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test[\"tag\"] = test[\"video_name\"].apply(extract_tag)\n",
    "# test[\"video_name\"] = test[\"video_name\"].apply(separate_video_name)\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98061a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 101\n",
    "# topNActs = train[\"tag\"].value_counts().nlargest(n).reset_index()[\"index\"].tolist()\n",
    "# train_new = train[train[\"tag\"].isin(topNActs)]\n",
    "# test_new = test[test[\"tag\"].isin(topNActs)]\n",
    "# train_new.shape, test_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c63961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_new = train_new.reset_index(drop=True)\n",
    "# test_new = test_new.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34b04ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def move_videos(df, output_dir):\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.mkdir(output_dir)\n",
    "#     for i in tqdm(range(df.shape[0])):\n",
    "#         videoFile = df['video_name'][i].split(\"/\")[-1]\n",
    "#         videoTag = df['tag'][i]\n",
    "#         videoPath = os.path.join(\"data\", videoFile)\n",
    "#         output_folder = os.path.join(output_dir, videoTag)\n",
    "#         if not os.path.exists(output_folder):\n",
    "#             os.mkdir(output_folder)\n",
    "#         shutil.copy2(videoPath, output_folder)\n",
    "#     print()\n",
    "#     print(f\"Total videos: {len(os.listdir(output_dir))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735e0280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move_videos(train_new, \"train\")\n",
    "# move_videos(test_new, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65b6e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VideoFrameGenerator - Simple Generator\n",
    "--------------------------------------\n",
    "A simple frame generator that takes distributed frames from\n",
    "videos. It is useful for videos that are scaled from frame 0 to end\n",
    "and that have no noise frames.\n",
    "\"\"\"\n",
    "\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "from math import floor\n",
    "from typing import Iterable, Optional\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import (ImageDataGenerator,\n",
    "                                                  img_to_array)\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "# from tensorflow.keras import backend as K\n",
    "# # Don't Show Warning Messages\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# import gc; gc.enable()\n",
    "\n",
    "log = logging.getLogger()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class VideoFrameGenerator(Sequence):  # pylint: disable=too-many-instance-attributes\n",
    "    \"\"\"\n",
    "    Create a generator that return batches of frames from video\n",
    "    # - rescale: float fraction to rescale pixel data (commonly 1/255.)\n",
    "    - nb_frames: int, number of frames to return for each sequence\n",
    "    - classes: list of str, classes to infer\n",
    "    - batch_size: int, batch size for each loop\n",
    "    - use_frame_cache: bool, use frame cache (may take a lot of memory for \\\n",
    "        large dataset)\n",
    "    - shape: tuple, target size of the frames\n",
    "    - shuffle: bool, randomize files\n",
    "    - transformation: ImageDataGenerator with transformations\n",
    "    - split: float, factor to split files and validation\n",
    "    - nb_channel: int, 1 or 3, to get grayscaled or RGB images\n",
    "    - glob_pattern: string, directory path with '{classname}' inside that \\\n",
    "        will be replaced by one of the class list\n",
    "    - use_header: bool, default to True to use video header to read the \\\n",
    "        frame count if possible\n",
    "    - seed: int, default to None, keep the seed value for split\n",
    "    You may use the \"classes\" property to retrieve the class list afterward.\n",
    "    The generator has that properties initialized:\n",
    "    - classes_count: number of classes that the generator manages\n",
    "    - files_count: number of video that the generator can provides\n",
    "    - classes: the given class list\n",
    "    - files: the full file list that the generator will use, this \\\n",
    "        is usefull if you want to remove some files that should not be \\\n",
    "        used by the generator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(  # pylint: disable=too-many-statements,too-many-locals,too-many-branches,too-many-arguments\n",
    "        self,\n",
    "        # rescale: float = 1 / 255.0,\n",
    "        nb_frames: int = 5,\n",
    "        classes: list = None,\n",
    "        batch_size: int = 16,\n",
    "        use_frame_cache: bool = False,\n",
    "        target_shape: tuple = (224, 224),\n",
    "        shuffle: bool = True,\n",
    "        transformation: Optional[ImageDataGenerator] = None,\n",
    "        split_test: float = None,\n",
    "        split_val: float = None,\n",
    "        nb_channel: int = 3,\n",
    "        glob_pattern: str = \"./videos/{classname}/*.avi\",\n",
    "        use_headers: bool = True,\n",
    "        seed=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        self.glob_pattern = glob_pattern\n",
    "\n",
    "        # should be only RGB or Grayscale\n",
    "        assert nb_channel in (1, 3)\n",
    "\n",
    "        if classes is None:\n",
    "            classes = self._discover_classes()\n",
    "\n",
    "        # we should have classes\n",
    "        if len(classes) == 0:\n",
    "            log.warn(\n",
    "                \"You didn't provide classes list or \"\n",
    "                \"we were not able to discover them from \"\n",
    "                \"your pattern.\\n\"\n",
    "                \"Please check if the path is OK, and if the glob \"\n",
    "                \"pattern is correct.\\n\"\n",
    "                \"See https://docs.python.org/3/library/glob.html\"\n",
    "            )\n",
    "\n",
    "        # shape size should be 2\n",
    "        assert len(target_shape) == 2\n",
    "\n",
    "        # split factor should be a propoer value\n",
    "        if split_val is not None:\n",
    "            assert 0.0 < split_val < 1.0\n",
    "\n",
    "        if split_test is not None:\n",
    "            assert 0.0 < split_test < 1.0\n",
    "\n",
    "        self.use_video_header = use_headers\n",
    "\n",
    "        # then we don't need None anymore\n",
    "        split_val = split_val if split_val is not None else 0.0\n",
    "        split_test = split_test if split_test is not None else 0.0\n",
    "\n",
    "        # be sure that classes are well ordered\n",
    "        classes.sort()\n",
    "\n",
    "        # self.rescale = rescale\n",
    "        self.classes = classes\n",
    "        self.batch_size = batch_size\n",
    "        self.nbframe = nb_frames\n",
    "        self.shuffle = shuffle\n",
    "        self.target_shape = target_shape\n",
    "        self.nb_channel = nb_channel\n",
    "        self.transformation = transformation\n",
    "        self.use_frame_cache = use_frame_cache\n",
    "\n",
    "        self._random_trans = []\n",
    "        self.__frame_cache = {}\n",
    "        self.files = []\n",
    "        self.validation = []\n",
    "        self.test = []\n",
    "\n",
    "        _validation_data = kwargs.get(\"_validation_data\", None)\n",
    "        _test_data = kwargs.get(\"_test_data\", None)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        if _validation_data is not None:\n",
    "            # we only need to set files here\n",
    "            self.files = _validation_data\n",
    "\n",
    "        elif _test_data is not None:\n",
    "            # we only need to set files here\n",
    "            self.files = _test_data\n",
    "        else:\n",
    "            self.__split_from_vals(\n",
    "                split_val, split_test, classes, shuffle, glob_pattern\n",
    "            )\n",
    "\n",
    "        # build indexes\n",
    "        self.files_count = len(self.files)\n",
    "        self.indexes = np.arange(self.files_count)\n",
    "        self.classes_count = len(classes)\n",
    "\n",
    "        # to initialize transformations and shuffle indices\n",
    "        if \"no_epoch_at_init\" not in kwargs:\n",
    "            self.on_epoch_end()\n",
    "\n",
    "        kind = \"train\"\n",
    "        if _validation_data is not None:\n",
    "            kind = \"validation\"\n",
    "        elif _test_data is not None:\n",
    "            kind = \"test\"\n",
    "\n",
    "        self._current = 0\n",
    "        self._framecounters = {}\n",
    "        print(\n",
    "            \"Total data: %d classes for %d files for %s\"\n",
    "            % (self.classes_count, self.files_count, kind)\n",
    "        )\n",
    "\n",
    "    def count_frames(self, cap, name, force_no_headers=False):\n",
    "        \"\"\"Count number of frame for video\n",
    "        if it's not possible with headers\"\"\"\n",
    "        if not force_no_headers and name in self._framecounters:\n",
    "            return self._framecounters[name]\n",
    "\n",
    "        total = cap.get(cv.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "        if force_no_headers or total < 0:\n",
    "            # headers not ok\n",
    "            total = 0\n",
    "            # TODO: we're unable to use CAP_PROP_POS_FRAME here\n",
    "            # so we open a new capture to not change the\n",
    "            # pointer position of \"cap\"\n",
    "            capture = cv.VideoCapture(name)\n",
    "            while True:\n",
    "                grabbed, _ = capture.read()\n",
    "                if not grabbed:\n",
    "                    # rewind and stop\n",
    "                    break\n",
    "                total += 1\n",
    "\n",
    "        # keep the result\n",
    "        self._framecounters[name] = total\n",
    "\n",
    "        return total\n",
    "\n",
    "    def __split_from_vals(self, split_val, split_test, classes, shuffle, glob_pattern):\n",
    "        \"\"\" Split validation and test set \"\"\"\n",
    "\n",
    "        if split_val == 0 or split_test == 0:\n",
    "            # no splitting, do the simplest thing\n",
    "            for cls in classes:\n",
    "                self.files += glob.glob(glob_pattern.format(classname=cls))\n",
    "            return\n",
    "\n",
    "        # else, there is some split to do\n",
    "        for cls in classes:\n",
    "            files = glob.glob(glob_pattern.format(classname=cls))\n",
    "            nbval = 0\n",
    "            nbtest = 0\n",
    "            info = []\n",
    "\n",
    "            # generate validation and test indexes\n",
    "            indexes = np.arange(len(files))\n",
    "\n",
    "            if shuffle:\n",
    "                np.random.shuffle(indexes)\n",
    "\n",
    "            nbtrain = 0\n",
    "            if 0.0 < split_val < 1.0:\n",
    "                nbval = int(split_val * len(files))\n",
    "                nbtrain = len(files) - nbval\n",
    "\n",
    "                # get some sample for validation_data\n",
    "                val = np.random.permutation(indexes)[:nbval]\n",
    "\n",
    "                # remove validation from train\n",
    "                indexes = np.array([i for i in indexes if i not in val])\n",
    "                self.validation += [files[i] for i in val]\n",
    "                info.append(\"validation count: %d\" % nbval)\n",
    "\n",
    "            if 0.0 < split_test < 1.0:\n",
    "                nbtest = int(split_test * nbtrain)\n",
    "                nbtrain = len(files) - nbval - nbtest\n",
    "\n",
    "                # get some sample for test_data\n",
    "                val_test = np.random.permutation(indexes)[:nbtest]\n",
    "\n",
    "                # remove test from train\n",
    "                indexes = np.array([i for i in indexes if i not in val_test])\n",
    "                self.test += [files[i] for i in val_test]\n",
    "                info.append(\"test count: %d\" % nbtest)\n",
    "\n",
    "            # and now, make the file list\n",
    "            self.files += [files[i] for i in indexes]\n",
    "            print(\"class %s, %s, train count: %d\" % (cls, \", \".join(info), nbtrain))\n",
    "\n",
    "    def _discover_classes(self):\n",
    "        pattern = os.path.realpath(self.glob_pattern)\n",
    "        pattern = re.escape(pattern)\n",
    "        pattern = pattern.replace(\"\\\\{classname\\\\}\", \"(.*?)\")\n",
    "        pattern = pattern.replace(\"\\\\*\", \".*\")\n",
    "\n",
    "        files = glob.glob(self.glob_pattern.replace(\"{classname}\", \"*\"))\n",
    "        classes = set()\n",
    "        for filename in files:\n",
    "            filename = os.path.realpath(filename)\n",
    "            classname = re.findall(pattern, filename)[0]\n",
    "            classes.add(classname)\n",
    "\n",
    "        return list(classes)\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\" Return next element\"\"\"\n",
    "        elem = self[self._current]\n",
    "        self._current += 1\n",
    "        if self._current == len(self):\n",
    "            self._current = 0\n",
    "            self.on_epoch_end()\n",
    "\n",
    "        return elem\n",
    "\n",
    "    # def get_validation_generator(self):\n",
    "    #     \"\"\" Return the validation generator if you've provided split factor \"\"\"\n",
    "    #     return self.__class__(\n",
    "    #         nb_frames=self.nbframe,\n",
    "    #         nb_channel=self.nb_channel,\n",
    "    #         target_shape=self.target_shape,\n",
    "    #         classes=self.classes,\n",
    "    #         batch_size=self.batch_size,\n",
    "    #         shuffle=self.shuffle,\n",
    "    #         # rescale=self.rescale,\n",
    "    #         glob_pattern=self.glob_pattern,\n",
    "    #         use_headers=self.use_video_header,\n",
    "    #         _validation_data=self.validation,\n",
    "    #     )\n",
    "\n",
    "    # def get_test_generator(self):\n",
    "    #     \"\"\" Return the validation generator if you've provided split factor \"\"\"\n",
    "    #     return self.__class__(\n",
    "    #         nb_frames=self.nbframe,\n",
    "    #         nb_channel=self.nb_channel,\n",
    "    #         target_shape=self.target_shape,\n",
    "    #         classes=self.classes,\n",
    "    #         batch_size=self.batch_size,\n",
    "    #         shuffle=self.shuffle,\n",
    "    #         # rescale=self.rescale,\n",
    "    #         glob_pattern=self.glob_pattern,\n",
    "    #         use_headers=self.use_video_header,\n",
    "    #         _test_data=self.test,\n",
    "    #     )\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\" Called by Keras after each epoch \"\"\"\n",
    "\n",
    "        if self.transformation is not None:\n",
    "            self._random_trans = []\n",
    "            for _ in range(self.files_count):\n",
    "                self._random_trans.append(\n",
    "                    self.transformation.get_random_transform(self.target_shape)\n",
    "                )\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)          \n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.files_count / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        classes = self.classes\n",
    "        shape = self.target_shape\n",
    "        nbframe = self.nbframe\n",
    "\n",
    "        labels = []\n",
    "        images = []\n",
    "        \n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "\n",
    "        transformation = None\n",
    "\n",
    "        for i in indexes:\n",
    "\n",
    "            video = self.files[i]\n",
    "            classname = self._get_classname(video)\n",
    "\n",
    "            # create a label array and set 1 to the right column\n",
    "            label = np.zeros(len(classes))\n",
    "            col = classes.index(classname)\n",
    "            label[col] = 1.0\n",
    "\n",
    "#             if video not in self.__frame_cache:\n",
    "#                 frames = self._get_frames(\n",
    "#                     video, nbframe, shape, force_no_headers=not self.use_video_header\n",
    "#                 )\n",
    "#                 if frames is None:\n",
    "#                     # avoid failure, nevermind that video...\n",
    "#                     continue\n",
    "\n",
    "#                 # add to cache\n",
    "#                 if self.use_frame_cache:\n",
    "#                     self.__frame_cache[video] = frames\n",
    "\n",
    "#             else:\n",
    "#                 frames = self.__frame_cache[video]\n",
    "            frames = self._get_frames(\n",
    "                    video, nbframe, shape, force_no_headers=not self.use_video_header\n",
    "                )\n",
    "\n",
    "            # apply transformation\n",
    "            # if provided\n",
    "            if self.transformation is not None:\n",
    "                transformation = self._random_trans[i]\n",
    "                frames = [\n",
    "                    self.transformation.apply_transform(frame, transformation)\n",
    "                    if transformation is not None\n",
    "                    else frame\n",
    "                    for frame in frames\n",
    "                ]\n",
    "\n",
    "            # add the sequence in batch\n",
    "            images.append(frames)\n",
    "            labels.append(label)\n",
    "\n",
    "        return np.array(images), np.array(labels)\n",
    "\n",
    "    def _get_classname(self, video: str) -> str:\n",
    "        \"\"\" Find classname from video filename following the pattern \"\"\"\n",
    "\n",
    "        # work with real path\n",
    "        video = os.path.realpath(video)\n",
    "        pattern = os.path.realpath(self.glob_pattern)\n",
    "\n",
    "        # remove special regexp chars\n",
    "        pattern = re.escape(pattern)\n",
    "\n",
    "        # get back \"*\" to make it \".*\" in regexp\n",
    "        pattern = pattern.replace(\"\\\\*\", \".*\")\n",
    "\n",
    "        # use {classname} as a capture\n",
    "        pattern = pattern.replace(\"\\\\{classname\\\\}\", \"(.*?)\")\n",
    "\n",
    "        # and find all occurence\n",
    "        classname = re.findall(pattern, video)[0]\n",
    "        return classname\n",
    "\n",
    "    def _get_frames(\n",
    "        self, video, nbframe, shape, force_no_headers=False\n",
    "    ) -> Optional[Iterable]:\n",
    "        cap = cv.VideoCapture(video)\n",
    "        total_frames = self.count_frames(cap, video, force_no_headers)\n",
    "        orig_total = total_frames\n",
    "\n",
    "        # if total_frames % 2 != 0:\n",
    "        #     total_frames += 1\n",
    "\n",
    "        frame_step = floor(total_frames / (nbframe - 1))\n",
    "        # print('frame step = ', frame_step)\n",
    "        # TODO: fix that, a tiny video can have a frame_step that is\n",
    "        # under 1\n",
    "        frame_step = max(1, frame_step)\n",
    "        frames = []\n",
    "        frame_i = 0\n",
    "\n",
    "        # while True:\n",
    "        #     grabbed, frame = cap.read()\n",
    "        #     if not grabbed:\n",
    "        #         break\n",
    "\n",
    "        #     # ifixit: increase frame index\n",
    "        #     frame_i += 1\n",
    "        for index in range(nbframe):\n",
    "            # print('index=', index)\n",
    "            frame_pos = index*(frame_step-1)\n",
    "            # print('frame pos=', frame_pos)\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_pos)\n",
    "            grabbed, frame = cap.read()\n",
    "            if not grabbed:\n",
    "                break\n",
    "\n",
    "            frame_i = frame_pos\n",
    "            # print('frame_i=',frame_i)\n",
    "            self.__add_and_convert_frame(\n",
    "                frame, frame_i, frames, orig_total, shape, frame_step\n",
    "            )\n",
    "\n",
    "            if len(frames) == nbframe:\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if not force_no_headers and len(frames) != nbframe:\n",
    "            # There is a problem here\n",
    "            # That means that frame count in header is wrong or broken,\n",
    "            # so we need to force the full read of video to get the right\n",
    "            # frame counter\n",
    "            return self._get_frames(video, nbframe, shape, force_no_headers=True)\n",
    "\n",
    "        if force_no_headers and len(frames) != nbframe:\n",
    "            # and if we really couldn't find the real frame counter\n",
    "            # so we return None. Sorry, nothing can be done...\n",
    "            log.error(\n",
    "                f\"Frame count is not OK for video {video}, \"\n",
    "                f\"{total_frames} total, {len(frames)} extracted\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        return np.array(frames)\n",
    "\n",
    "    def __add_and_convert_frame(  # pylint: disable=too-many-arguments\n",
    "        self, frame, frame_i, frames, orig_total, shape, frame_step\n",
    "    ):\n",
    "        #frame_i += 1\n",
    "        # if frame_i in (1, orig_total) or frame_i % frame_step == 0:\n",
    "        # crop center\n",
    "        frame = self.__crop_center_square(frame)\n",
    "        # resize\n",
    "        frame = cv.resize(frame, shape)\n",
    "\n",
    "        # use RGB or Grayscale ?\n",
    "        frame = (\n",
    "            cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "            if self.nb_channel == 3\n",
    "            else cv.cvtColor(frame, cv.COLOR_RGB2GRAY)\n",
    "        )\n",
    "\n",
    "        # to np\n",
    "        frame = img_to_array(frame)# * self.rescale\n",
    "\n",
    "        # keep frame\n",
    "        # print('append frame at frame_i= ', frame_i)\n",
    "        frames.append(frame)\n",
    "\n",
    "    def __crop_center_square(\n",
    "        self, frame\n",
    "    ):\n",
    "        y, x = frame.shape[0:2]\n",
    "        min_dim = min(y, x)\n",
    "        start_x = (x // 2) - (min_dim // 2)\n",
    "        start_y = (y // 2) - (min_dim // 2)\n",
    "        return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad6235af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "Total data: 101 classes for 9537 files for train\n",
      "Total data: 101 classes for 3783 files for train\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "# from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "# from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "# from tensorflow.keras.applications.densenet import preprocess_input\n",
    "\n",
    "\n",
    "# from keras_video import VideoFrameGenerator\n",
    "# use sub directories names as classes\n",
    "classes = [i.split(os.path.sep)[1] for i in glob.glob('train/*')]\n",
    "classes.sort()\n",
    "print(len(classes))\n",
    "\n",
    "# some global params\n",
    "SIZE = (224, 224)\n",
    "CHANNELS = 3\n",
    "NBFRAME = 4 #5\n",
    "BS = 8\n",
    "#\n",
    "MAX_SEQ_LENGTH = NBFRAME#override max sequence length\n",
    "NUM_FEATURES = 2048#1920\n",
    "#\n",
    "INSHAPE=(NBFRAME,) + SIZE + (CHANNELS,) # (5, 112, 112, 3)\n",
    "# pattern to get videos and classes\n",
    "glob_train_pattern='train/{classname}/*'\n",
    "glob_test_pattern='test/{classname}/*'\n",
    "# for data augmentation\n",
    "data_train_aug = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    zoom_range=.1,\n",
    "    # horizontal_flip=True,\n",
    "    rotation_range=8,\n",
    "    width_shift_range=.2,\n",
    "    height_shift_range=.2,\n",
    "    preprocessing_function=preprocess_input,\n",
    "    )\n",
    "\n",
    "data_test_aug = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    )\n",
    "# Create video frame generator\n",
    "train = VideoFrameGenerator(\n",
    "    classes=classes, \n",
    "    glob_pattern=glob_train_pattern,\n",
    "    nb_frames=NBFRAME,\n",
    "    # split=.33, \n",
    "    shuffle=True,\n",
    "    batch_size=BS,\n",
    "    target_shape=SIZE,\n",
    "    nb_channel=CHANNELS,\n",
    "    transformation=data_train_aug,\n",
    "    use_frame_cache=True)\n",
    "\n",
    "# Create video frame generator\n",
    "test = VideoFrameGenerator(\n",
    "    classes=classes, \n",
    "    glob_pattern=glob_test_pattern,\n",
    "    nb_frames=NBFRAME,\n",
    "    # split=.33, \n",
    "    shuffle=False,\n",
    "    batch_size=BS,\n",
    "    target_shape=SIZE,\n",
    "    nb_channel=CHANNELS,\n",
    "    transformation=data_test_aug,\n",
    "    use_frame_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a0cea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, \\\n",
    "    MaxPool2D, GlobalMaxPool2D\n",
    "from tensorflow.keras.models import Model\n",
    "# from tf.keras.applications.mobilenet import preprocess_input\n",
    "\n",
    "\n",
    "def build_convnet(shape=(224, 224, 3)):\n",
    "    f1_base = tf.keras.applications.ResNet152(weights='imagenet', include_top=False, input_shape=shape)\n",
    "    # f1_base = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=shape)\n",
    "    # f1_base = tf.keras.applications.MobileNet(weights='imagenet', include_top=False, input_shape=shape)\n",
    "#     f1_base = tf.keras.applications.DenseNet201(weights='imagenet', include_top=False, input_shape=shape)  \n",
    "    f1_x = f1_base.output\n",
    "\n",
    "    # #frozen layers    \n",
    "    # for layer in f1_base.layers:\n",
    "    #     layer.trainable = False  \n",
    "\n",
    "    f1_x = tf.keras.layers.GlobalAveragePooling2D()(f1_x)\n",
    "\n",
    "    model_1 = Model(inputs=[f1_base.input],outputs=[f1_x])        \n",
    "\n",
    "    return model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f51a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "\n",
    "class PositionalEmbedding(tensorflow.keras.layers.Layer):\n",
    "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.position_embeddings = tensorflow.keras.layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
    "        length = tf.shape(inputs)[1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return inputs + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
    "        return mask\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'sequence_length': self.sequence_length,\n",
    "            'output_dim': self.output_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4766f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(tensorflow.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = tensorflow.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
    "        )\n",
    "        self.dense_proj = tensorflow.keras.Sequential(\n",
    "            [tensorflow.keras.layers.Dense(dense_dim, activation=tf.nn.gelu), tensorflow.keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = tensorflow.keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = tensorflow.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'dense_dim': self.dense_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "        })\n",
    "        return config    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3261b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TimeDistributed, GRU, Dense, Dropout, GaussianNoise, GlobalMaxPooling1D\n",
    "\n",
    "def adv_action_model(shape=(4, 224, 224, 3), nbout=3):\n",
    "    sequence_length = MAX_SEQ_LENGTH\n",
    "    embed_dim = NUM_FEATURES\n",
    "    dense_dim = 64\n",
    "    num_heads = 4\n",
    "    classes = 101 #len(label_processor.get_vocabulary())\n",
    "    \n",
    "    # Create our convnet with (112, 112, 3) input shape\n",
    "    convnet = build_convnet(shape[1:])\n",
    "\n",
    "    # for layer in convnet.layers:\n",
    "    #     print(layer.name, ': ', layer.trainable)   \n",
    "    \n",
    "    # then create our final model\n",
    "    model = tf.keras.Sequential()\n",
    "    # add the convnet with (5, 112, 112, 3) shape\n",
    "    model.add(TimeDistributed(convnet, input_shape=shape))\n",
    "#     # here, you can also use GRU or LSTM\n",
    "#     model.add(GRU(2048))\n",
    "    model.add(PositionalEmbedding(\n",
    "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
    "    ))\n",
    "    model.add(TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\"))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    #Regularization with noise\n",
    "    model.add(GaussianNoise(0.1))\n",
    "    # and finally, we make a decision network\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(.4))\n",
    "    model.add(Dense(nbout, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4ce3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSHAPE=(NBFRAME,) + SIZE + (CHANNELS,) # (5, 112, 112, 3)\n",
    "\n",
    "# print(INSHAPE)\n",
    "# print(len(classes))\n",
    "# model = adv_action_model(INSHAPE, len(classes))\n",
    "# # optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "# optimizer = tf.keras.optimizers.SGD(0.01)\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer,\n",
    "#     'categorical_crossentropy',\n",
    "#     metrics=['acc'],\n",
    "#     run_eagerly=True\n",
    "# )\n",
    "\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c72b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "\n",
    "# EPOCHS=60\n",
    "# # create a \"chkp\" directory before to run that\n",
    "# # because ModelCheckpoint will write models inside\n",
    "# callbacks = [\n",
    "#     # tf.keras.callbacks.ReduceLROnPlateau(verbose=1),\n",
    "#     # tf.keras.callbacks.ModelCheckpoint(\n",
    "#     #     'chkp/weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "#     #     verbose=1),\n",
    "# ]\n",
    "# model.fit(\n",
    "#     train,\n",
    "#     validation_data=test,\n",
    "#     verbose=1,\n",
    "    \n",
    "#     epochs=EPOCHS,\n",
    "# #     callbacks=callbacks,\n",
    "#     # workers=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734b67fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c00fb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of GPUs: 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 16:36:46.887087: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-28 16:36:47.429308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10415 MB memory:  -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "import time, os\n",
    "from math import ceil\n",
    "import random\n",
    "\n",
    "model_txt = 'st'\n",
    "# Helper: Save the model.\n",
    "savedfilename = os.path.join('checkpoints', 'ucf101_2_ResNet152Transformer_set1_D1.hdf5')\n",
    "savedfilename_best = os.path.join('checkpoints', 'ucf101_2_ResNet152Transformer_set1_D1_best.hdf5')\n",
    "savedfilename_pre = os.path.join('checkpoints', 'ucf101_2_ResNet152Transformer_set1_D1_pre.hdf5')\n",
    "\n",
    "checkpointer = ModelCheckpoint(savedfilename,\n",
    "                          monitor='val_accuracy', verbose=1, \n",
    "                          save_best_only=False, mode='max',save_weights_only=True)########\n",
    "\n",
    "# Helper: TensorBoard\n",
    "tb = TensorBoard(log_dir=os.path.join('svhn_output', 'logs', model_txt))\n",
    "\n",
    "# Helper: Save results.\n",
    "timestamp = time.time()\n",
    "csv_logger = CSVLogger(os.path.join('svhn_output', 'logs', model_txt + '-' + 'training-' + \\\n",
    "    str(timestamp) + '.log'))\n",
    "\n",
    "earlystopping = EarlyStoppingByAccVal(monitor='val_accuracy', value=0.9900, verbose=1)\n",
    "\n",
    "def rand_scheduler(epoch, lr):\n",
    "    # rnd_lr = 10**(random.uniform(np.log10((1e-5)),np.log10((1e-1))))\n",
    "#     if epoch < 30:\n",
    "#         rnd_lr = 1e-2\n",
    "#     else:    \n",
    "#         rnd_lr = 1e-3\n",
    "    rnd_lr = lr\n",
    "    print('random lr = ', rnd_lr)\n",
    "    return rnd_lr\n",
    "\n",
    "epochs = 20##!!!\n",
    "lr = 1e-3\n",
    "# decay = lr/epochs\n",
    "# optimizer = Adam(lr=lr, decay=decay)\n",
    "# optimizer = Adam(lr=lr)\n",
    "optimizer = SGD(learning_rate=lr)\n",
    "\n",
    "# train on multiple-gpus\n",
    "# Create a MirroredStrategy.\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of GPUs: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Open a strategy scope.\n",
    "with strategy.scope():\n",
    "    # Everything that creates variables should be under the strategy scope.\n",
    "    # In general this is only model construction & `compile()`.\n",
    "    model_mul = adv_action_model(INSHAPE, len(classes))\n",
    "    model_mul.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    # save initial model\n",
    "    model_mul.save_weights(savedfilename)\n",
    "    model_mul.save_weights(savedfilename_best)\n",
    "    model_mul.save_weights(savedfilename_pre)\n",
    "    \n",
    "# step_size_train=ceil(train_set.n/train_set.batch_size)\n",
    "# step_size_valid=ceil(valid_set.n/valid_set.batch_size)\n",
    "# step_size_test=ceil(testing_set.n//testing_set.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2f9502b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 16:37:06.314104: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_44302\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\020FlatMapDataset:1\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  0.0010000000474974513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 16:37:57.954681: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n",
      "2022-09-28 16:37:58.222237: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-09-28 16:37:58.222650: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-09-28 16:37:58.222681: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2022-09-28 16:37:58.223149: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-09-28 16:37:58.223216: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - ETA: 0s - loss: 3.2638 - accuracy: 0.2720"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 16:52:17.636695: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_111138\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021FlatMapDataset:23\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_2_ResNet152Transformer_set1_D1.hdf5\n",
      "1192/1192 [==============================] - 1068s 852ms/step - loss: 3.2638 - accuracy: 0.2720 - val_loss: 1.7218 - val_accuracy: 0.5646 - lr: 0.0010\n",
      "0\n",
      "flg_i 0 flag 0\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 1\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 1\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 1\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_6591/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 44934.41597958271), (1, 0), (2, 10434.889377856776), (3, 11146.688070291772)]\n",
      "distances  [(1, 0), (2, 10434.889377856776), (3, 11146.688070291772), (0, 44934.41597958271)]\n",
      "neighbors ids  [1, 2, 3, 0]\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.5646186470985413\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.0712394043803215\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.1787605881690979\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.539459764957428\n",
      "neighbor best  [(1, 0.5646186470985413), (0, 0.539459764957428), (3, 0.1787605881690979), (2, 0.0712394043803215)]\n",
      "name_file_neighbor_best  ucf101_2_ResNet152Transformer_set1_D1_best.hdf5\n",
      "u  0.2\n",
      "distance_ij  10434.889377856776\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:235: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sum_particle_tmp -= M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  10\n",
      "distance_ij  11146.688070291772\n",
      "tmp_lr  0.001\n",
      "u  0.2\n",
      "distance_ij  44934.41597958271\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:250: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 0  r1= 0.5326777384258465  r2= 0.8404017119451143  current acc= 0.5646186470985413  local best= 0.5646186470985413  neighbor index= 1  neighbor best= 0.5646186470985413\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 17:08:48.954295: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_179215\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021FlatMapDataset:45\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  0.0010000000474974513\n",
      "1192/1192 [==============================] - ETA: 0s - loss: 1.2981 - accuracy: 0.6582"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 17:22:33.059022: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_185371\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021FlatMapDataset:67\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_2_ResNet152Transformer_set1_D1.hdf5\n",
      "1192/1192 [==============================] - 980s 822ms/step - loss: 1.2981 - accuracy: 0.6582 - val_loss: 1.0248 - val_accuracy: 0.7198 - lr: 0.0010\n",
      "0\n",
      "flg_i 0 flag 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 1\n",
      "flg_i 3 flag 1\n",
      "\n",
      "\n",
      "flg_i 0 flag 1.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 1\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 1.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_6591/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 25505.949149509637), (1, 0), (2, 10128.873009113502), (3, 14505.942466052928)]\n",
      "distances  [(1, 0), (2, 10128.873009113502), (3, 14505.942466052928), (0, 25505.949149509637)]\n",
      "neighbors ids  [1, 2, 3, 0]\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.7198092937469482\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.1753177940845489\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.5564088821411133\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.6144067645072937\n",
      "neighbor best  [(1, 0.7198092937469482), (0, 0.6144067645072937), (3, 0.5564088821411133), (2, 0.1753177940845489)]\n",
      "name_file_neighbor_best  ucf101_2_ResNet152Transformer_set1_D1_best.hdf5\n",
      "u  0.2\n",
      "distance_ij  10128.873009113502\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:235: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sum_particle_tmp -= M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  10\n",
      "distance_ij  14505.942466052928\n",
      "tmp_lr  0.001\n",
      "u  0.2\n",
      "distance_ij  25505.949149509637\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:250: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 1  r1= 0.3363673076350834  r2= 0.3741309657774795  current acc= 0.7198092937469482  local best= 0.7198092937469482  neighbor index= 1  neighbor best= 0.7198092937469482\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 17:39:13.846222: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_245820\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021FlatMapDataset:89\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  0.0010000000474974513\n",
      "1192/1192 [==============================] - ETA: 0s - loss: 0.6852 - accuracy: 0.8067"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 17:53:28.392847: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_251976\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:111\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_2_ResNet152Transformer_set1_D1.hdf5\n",
      "1192/1192 [==============================] - 1009s 847ms/step - loss: 0.6852 - accuracy: 0.8067 - val_loss: 0.8603 - val_accuracy: 0.7582 - lr: 0.0010\n",
      "0\n",
      "flg_i 0 flag 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_6591/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 32861.21730155184), (1, 0), (2, 4463.479932622072), (3, 7168.618843603161)]\n",
      "distances  [(1, 0), (2, 4463.479932622072), (3, 7168.618843603161), (0, 32861.21730155184)]\n",
      "neighbors ids  [1, 2, 3, 0]\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.758209764957428\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.585540235042572\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.5564088821411133\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.7211334705352783\n",
      "neighbor best  [(1, 0.758209764957428), (0, 0.7211334705352783), (2, 0.585540235042572), (3, 0.5564088821411133)]\n",
      "name_file_neighbor_best  ucf101_2_ResNet152Transformer_set1_D1_best.hdf5\n",
      "u  0.2\n",
      "distance_ij  4463.479932622072\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:235: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sum_particle_tmp -= M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  10\n",
      "distance_ij  7168.618843603161\n",
      "tmp_lr  0.001\n",
      "u  0.2\n",
      "distance_ij  32861.21730155184\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:250: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 2  r1= 0.23275319253034854  r2= 0.9041300637080972  current acc= 0.758209764957428  local best= 0.758209764957428  neighbor index= 1  neighbor best= 0.758209764957428\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 18:07:50.811748: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_312425\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:133\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  0.0010000000474974513\n",
      "1192/1192 [==============================] - ETA: 0s - loss: 0.4385 - accuracy: 0.8768"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 18:21:43.685258: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_318581\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:155\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_2_ResNet152Transformer_set1_D1.hdf5\n",
      "1192/1192 [==============================] - 991s 832ms/step - loss: 0.4385 - accuracy: 0.8768 - val_loss: 0.7626 - val_accuracy: 0.7855 - lr: 0.0010\n",
      "0\n",
      "flg_i 0 flag 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 1\n",
      "flg_i 3 flag 1\n",
      "\n",
      "\n",
      "flg_i 0 flag 1.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_6591/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 28455.69669188962), (1, 0), (2, 9878.8495817813), (3, 179270.25133097073)]\n",
      "distances  [(1, 0), (2, 9878.8495817813), (0, 28455.69669188962), (3, 179270.25133097073)]\n",
      "neighbors ids  [1, 2, 0, 3]\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.7854872941970825\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.7171609997749329\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.7211334705352783\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.5564088821411133\n",
      "neighbor best  [(1, 0.7854872941970825), (0, 0.7211334705352783), (2, 0.7171609997749329), (3, 0.5564088821411133)]\n",
      "name_file_neighbor_best  ucf101_2_ResNet152Transformer_set1_D1_best.hdf5\n",
      "u  0.2\n",
      "distance_ij  9878.8495817813\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:235: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sum_particle_tmp -= M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  0.2\n",
      "distance_ij  28455.69669188962\n",
      "tmp_lr  0.001\n",
      "u  10\n",
      "distance_ij  179270.25133097073\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:250: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 3  r1= 0.14659517334588623  r2= 0.008905780767662907  current acc= 0.7854872941970825  local best= 0.7854872941970825  neighbor index= 1  neighbor best= 0.7854872941970825\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 18:37:19.284072: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_379030\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:177\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  0.0010000000474974513\n",
      "1192/1192 [==============================] - ETA: 0s - loss: 0.2964 - accuracy: 0.9179"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 18:51:31.545231: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_385186\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:199\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_2_ResNet152Transformer_set1_D1.hdf5\n",
      "1192/1192 [==============================] - 1005s 842ms/step - loss: 0.2964 - accuracy: 0.9179 - val_loss: 0.7423 - val_accuracy: 0.7908 - lr: 0.0010\n",
      "0\n",
      "flg_i 0 flag 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 1\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_6591/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 25225.054971300044), (1, 0), (2, 6485.309561253724), (3, 98095.15774746505)]\n",
      "distances  [(1, 0), (2, 6485.309561253724), (0, 25225.054971300044), (3, 98095.15774746505)]\n",
      "neighbors ids  [1, 2, 0, 3]\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.7907838821411133\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.7703919410705566\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.7211334705352783\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.5564088821411133\n",
      "neighbor best  [(1, 0.7907838821411133), (2, 0.7703919410705566), (0, 0.7211334705352783), (3, 0.5564088821411133)]\n",
      "name_file_neighbor_best  ucf101_2_ResNet152Transformer_set1_D1_best.hdf5\n",
      "u  0.2\n",
      "distance_ij  6485.309561253724\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:235: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sum_particle_tmp -= M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  0.2\n",
      "distance_ij  25225.054971300044\n",
      "tmp_lr  0.001\n",
      "u  10\n",
      "distance_ij  98095.15774746505\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:250: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 4  r1= 0.08555737806960328  r2= 0.9102936156593247  current acc= 0.7907838821411133  local best= 0.7907838821411133  neighbor index= 1  neighbor best= 0.7907838821411133\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 19:06:02.672428: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_445635\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:221\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  0.0010000000474974513\n",
      "1192/1192 [==============================] - ETA: 0s - loss: 0.2132 - accuracy: 0.9438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 19:20:10.449785: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_451791\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:243\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_2_ResNet152Transformer_set1_D1.hdf5\n",
      "1192/1192 [==============================] - 1014s 850ms/step - loss: 0.2132 - accuracy: 0.9438 - val_loss: 0.6980 - val_accuracy: 0.8038 - lr: 0.0010\n",
      "0\n",
      "flg_i 0 flag 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 1\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 1.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_6591/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 34021.27722149795), (1, 0), (2, 10796.956385527808), (3, 53209.46947109244)]\n",
      "distances  [(1, 0), (2, 10796.956385527808), (0, 34021.27722149795), (3, 53209.46947109244)]\n",
      "neighbors ids  [1, 2, 0, 3]\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.8037605881690979\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.7862817645072937\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.7229872941970825\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.5564088821411133\n",
      "neighbor best  [(1, 0.8037605881690979), (2, 0.7862817645072937), (0, 0.7229872941970825), (3, 0.5564088821411133)]\n",
      "name_file_neighbor_best  ucf101_2_ResNet152Transformer_set1_D1_best.hdf5\n",
      "u  0.2\n",
      "distance_ij  10796.956385527808\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:235: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sum_particle_tmp -= M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  0.2\n",
      "distance_ij  34021.27722149795\n",
      "tmp_lr  0.001\n",
      "u  10\n",
      "distance_ij  53209.46947109244\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:250: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 5  r1= 0.7108310557141703  r2= 0.4707113359638442  current acc= 0.8037605881690979  local best= 0.8037605881690979  neighbor index= 1  neighbor best= 0.8037605881690979\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 19:35:52.209840: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_512240\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:265\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  0.0010000000474974513\n",
      "1192/1192 [==============================] - ETA: 0s - loss: 0.1543 - accuracy: 0.9578"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 19:49:49.539825: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_518396\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:287\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_2_ResNet152Transformer_set1_D1.hdf5\n",
      "1192/1192 [==============================] - 992s 832ms/step - loss: 0.1543 - accuracy: 0.9578 - val_loss: 0.6900 - val_accuracy: 0.8059 - lr: 0.0010\n",
      "0\n",
      "flg_i 0 flag 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 1\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 1\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_6591/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 33204.28531522807), (1, 0), (2, 6428.020126940145), (3, 25075.221452618614)]\n",
      "distances  [(1, 0), (2, 6428.020126940145), (3, 25075.221452618614), (0, 33204.28531522807)]\n",
      "neighbors ids  [1, 2, 3, 0]\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.8058792352676392\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.8029661178588867\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.7722457647323608\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.7229872941970825\n",
      "neighbor best  [(1, 0.8058792352676392), (2, 0.8029661178588867), (3, 0.7722457647323608), (0, 0.7229872941970825)]\n",
      "name_file_neighbor_best  ucf101_2_ResNet152Transformer_set1_D1_best.hdf5\n",
      "u  0.2\n",
      "distance_ij  6428.020126940145\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:235: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sum_particle_tmp -= M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  10\n",
      "distance_ij  25075.221452618614\n",
      "tmp_lr  0.001\n",
      "u  0.2\n",
      "distance_ij  33204.28531522807\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:250: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 6  r1= 0.09944529057395912  r2= 0.33122549802086854  current acc= 0.8058792352676392  local best= 0.8058792352676392  neighbor index= 1  neighbor best= 0.8058792352676392\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 20:05:24.330649: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_578845\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:309\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  0.0010000000474974513\n",
      "1192/1192 [==============================] - ETA: 0s - loss: 0.1210 - accuracy: 0.9706"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 20:19:36.602101: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_585001\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:331\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_2_ResNet152Transformer_set1_D1.hdf5\n",
      "1192/1192 [==============================] - 1006s 844ms/step - loss: 0.1210 - accuracy: 0.9706 - val_loss: 0.6407 - val_accuracy: 0.8244 - lr: 0.0010\n",
      "0\n",
      "flg_i 0 flag 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_6591/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 117067.05273814025), (1, 0), (2, 6956.071461619872), (3, 16617.30233556435)]\n",
      "distances  [(1, 0), (2, 6956.071461619872), (3, 16617.30233556435), (0, 117067.05273814025)]\n",
      "neighbors ids  [1, 2, 3, 0]\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.8244173526763916\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.8132944703102112\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.8019067645072937\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.7229872941970825\n",
      "neighbor best  [(1, 0.8244173526763916), (2, 0.8132944703102112), (3, 0.8019067645072937), (0, 0.7229872941970825)]\n",
      "name_file_neighbor_best  ucf101_2_ResNet152Transformer_set1_D1_best.hdf5\n",
      "u  0.2\n",
      "distance_ij  6956.071461619872\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:235: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sum_particle_tmp -= M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  10\n",
      "distance_ij  16617.30233556435\n",
      "tmp_lr  0.001\n",
      "u  0.2\n",
      "distance_ij  117067.05273814025\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:250: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 7  r1= 0.6681730982884564  r2= 0.11976031985761582  current acc= 0.8244173526763916  local best= 0.8244173526763916  neighbor index= 1  neighbor best= 0.8244173526763916\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 20:34:06.290676: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_645450\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:353\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  0.0010000000474974513\n",
      "1192/1192 [==============================] - ETA: 0s - loss: 0.0950 - accuracy: 0.9750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 20:48:26.490725: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_651606\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:375\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_2_ResNet152Transformer_set1_D1.hdf5\n",
      "1192/1192 [==============================] - 1027s 862ms/step - loss: 0.0950 - accuracy: 0.9750 - val_loss: 0.6460 - val_accuracy: 0.8197 - lr: 0.0010\n",
      "0\n",
      "flg_i 0 flag 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 1\n",
      "flg_i 3 flag 1\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_6591/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 24115.56249818506), (1, 0), (2, 7151.7234592238965), (3, 11349.983097412462)]\n",
      "distances  [(1, 0), (2, 7151.7234592238965), (3, 11349.983097412462), (0, 24115.56249818506)]\n",
      "neighbors ids  [1, 2, 3, 0]\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.8244173526763916\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.8154131174087524\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.8193855881690979\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.7229872941970825\n",
      "neighbor best  [(1, 0.8244173526763916), (3, 0.8193855881690979), (2, 0.8154131174087524), (0, 0.7229872941970825)]\n",
      "name_file_neighbor_best  ucf101_2_ResNet152Transformer_set1_D1_best.hdf5\n",
      "u  0.2\n",
      "distance_ij  7151.7234592238965\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:235: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sum_particle_tmp -= M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  10\n",
      "distance_ij  11349.983097412462\n",
      "tmp_lr  0.001\n",
      "u  0.2\n",
      "distance_ij  24115.56249818506\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:250: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 8  r1= 0.7948925925166165  r2= 0.41359894895578164  current acc= 0.819650411605835  local best= 0.8244173526763916  neighbor index= 1  neighbor best= 0.8244173526763916\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 21:03:13.958638: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_708582\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:397\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  0.0010000000474974513\n",
      "1192/1192 [==============================] - ETA: 0s - loss: 0.0852 - accuracy: 0.9784"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 21:17:43.102040: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_714738\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:419\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_2_ResNet152Transformer_set1_D1.hdf5\n",
      "1192/1192 [==============================] - 1045s 877ms/step - loss: 0.0852 - accuracy: 0.9784 - val_loss: 0.6385 - val_accuracy: 0.8247 - lr: 0.0010\n",
      "0\n",
      "flg_i 0 flag 0\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 1\n",
      "flg_i 3 flag 1\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_6591/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 23843.08241736673), (1, 0), (2, 6418.052063135029), (3, 5889.821889517367)]\n",
      "distances  [(1, 0), (3, 5889.821889517367), (2, 6418.052063135029), (0, 23843.08241736673)]\n",
      "neighbors ids  [1, 3, 2, 0]\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.8246821761131287\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.8294491767883301\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.8215042352676392\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.7229872941970825\n",
      "neighbor best  [(3, 0.8294491767883301), (1, 0.8246821761131287), (2, 0.8215042352676392), (0, 0.7229872941970825)]\n",
      "name_file_neighbor_best  ucf101_4_ResNet152Transformer_set1_D1_best.hdf5\n",
      "u  10\n",
      "distance_ij  5889.821889517367\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:235: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sum_particle_tmp -= M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  0.2\n",
      "distance_ij  6418.052063135029\n",
      "tmp_lr  0.001\n",
      "u  0.2\n",
      "distance_ij  23843.08241736673\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:250: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 9  r1= 0.691241099401916  r2= 0.9836431938230611  current acc= 0.8246821761131287  local best= 0.8246821761131287  neighbor index= 3  neighbor best= 0.8294491767883301\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 21:32:40.843717: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_775187\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:441\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  0.0010000000474974513\n",
      "1192/1192 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9802"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 21:46:57.816101: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_781343\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:463\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_2_ResNet152Transformer_set1_D1.hdf5\n",
      "1192/1192 [==============================] - 1021s 856ms/step - loss: 0.0734 - accuracy: 0.9802 - val_loss: 0.6080 - val_accuracy: 0.8257 - lr: 0.0010\n",
      "0\n",
      "flg_i 0 flag 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 1\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 1.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_6591/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 14948.930234830728), (1, 0), (2, 5213.234875648875), (3, 20121.29138458491)]\n",
      "distances  [(1, 0), (2, 5213.234875648875), (0, 14948.930234830728), (3, 20121.29138458491)]\n",
      "neighbors ids  [1, 2, 0, 3]\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.8257415294647217\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.8257415294647217\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.7229872941970825\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.8294491767883301\n",
      "neighbor best  [(3, 0.8294491767883301), (1, 0.8257415294647217), (2, 0.8257415294647217), (0, 0.7229872941970825)]\n",
      "name_file_neighbor_best  ucf101_4_ResNet152Transformer_set1_D1_best.hdf5\n",
      "u  0.2\n",
      "distance_ij  5213.234875648875\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:235: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sum_particle_tmp -= M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  0.2\n",
      "distance_ij  14948.930234830728\n",
      "tmp_lr  0.001\n",
      "u  10\n",
      "distance_ij  20121.29138458491\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:250: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 10  r1= 0.6548295620167413  r2= 0.409663120809842  current acc= 0.8257415294647217  local best= 0.8257415294647217  neighbor index= 3  neighbor best= 0.8294491767883301\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 22:02:53.807424: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_841792\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:485\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  0.0010000000474974513\n",
      "1192/1192 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9836"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 22:17:12.684704: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_847948\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:507\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_2_ResNet152Transformer_set1_D1.hdf5\n",
      "1192/1192 [==============================] - 1039s 870ms/step - loss: 0.0686 - accuracy: 0.9836 - val_loss: 0.6049 - val_accuracy: 0.8334 - lr: 0.0010\n",
      "\n",
      "\n",
      "0e-read the file .... 1\n",
      "flg_i 0 flag 0\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_6591/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 24994.587862949473), (1, 0), (2, 3635.33638206846), (3, 14770.558156841778)]\n",
      "distances  [(1, 0), (2, 3635.33638206846), (3, 14770.558156841778), (0, 24994.587862949473)]\n",
      "neighbors ids  [1, 2, 3, 0]\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.8334215879440308\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.8257415294647217\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.8294491767883301\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.7229872941970825\n",
      "neighbor best  [(1, 0.8334215879440308), (3, 0.8294491767883301), (2, 0.8257415294647217), (0, 0.7229872941970825)]\n",
      "name_file_neighbor_best  ucf101_2_ResNet152Transformer_set1_D1_best.hdf5\n",
      "u  0.2\n",
      "distance_ij  3635.33638206846\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:235: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sum_particle_tmp -= M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  10\n",
      "distance_ij  14770.558156841778\n",
      "tmp_lr  0.001\n",
      "u  0.2\n",
      "distance_ij  24994.587862949473\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:250: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 11  r1= 0.7336211244620882  r2= 0.5730007437632316  current acc= 0.8334215879440308  local best= 0.8334215879440308  neighbor index= 1  neighbor best= 0.8334215879440308\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 22:31:19.011919: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_908397\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:529\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  0.0010000000474974513\n",
      "1192/1192 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.9864"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 22:45:44.732358: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_914553\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:551\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_2_ResNet152Transformer_set1_D1.hdf5\n",
      "1192/1192 [==============================] - 1030s 863ms/step - loss: 0.0560 - accuracy: 0.9864 - val_loss: 0.6208 - val_accuracy: 0.8276 - lr: 0.0010\n",
      "0\n",
      "flg_i 0 flag 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 1\n",
      "flg_i 3 flag 1\n",
      "\n",
      "\n",
      "flg_i 0 flag 1.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 1\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_6591/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 18076.46911222724), (1, 0), (2, 4047.9326905228095), (3, 22052.244302828578)]\n",
      "distances  [(1, 0), (2, 4047.9326905228095), (0, 18076.46911222724), (3, 22052.244302828578)]\n",
      "neighbors ids  [1, 2, 0, 3]\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.8334215879440308\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.8328919410705566\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.7229872941970825\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.8294491767883301\n",
      "neighbor best  [(1, 0.8334215879440308), (2, 0.8328919410705566), (3, 0.8294491767883301), (0, 0.7229872941970825)]\n",
      "name_file_neighbor_best  ucf101_2_ResNet152Transformer_set1_D1_best.hdf5\n",
      "u  0.2\n",
      "distance_ij  4047.9326905228095\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:235: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sum_particle_tmp -= M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  0.2\n",
      "distance_ij  18076.46911222724\n",
      "tmp_lr  0.001\n",
      "u  10\n",
      "distance_ij  22052.244302828578\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:250: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 12  r1= 0.7688198743531811  r2= 0.612155242303031  current acc= 0.8275953531265259  local best= 0.8334215879440308  neighbor index= 1  neighbor best= 0.8334215879440308\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 23:01:24.688611: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_971529\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:573\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  0.0010000000474974513\n",
      "1192/1192 [==============================] - ETA: 0s - loss: 0.0524 - accuracy: 0.9885"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 23:15:55.722580: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_977685\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:595\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_2_ResNet152Transformer_set1_D1.hdf5\n",
      "1192/1192 [==============================] - 1046s 877ms/step - loss: 0.0524 - accuracy: 0.9885 - val_loss: 0.6048 - val_accuracy: 0.8340 - lr: 0.0010\n",
      "0\n",
      "flg_i 0 flag 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 1\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_6591/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 31629.245738059057), (1, 0), (2, 5669.348124077805), (3, 16507.36638778964)]\n",
      "distances  [(1, 0), (2, 5669.348124077805), (3, 16507.36638778964), (0, 31629.245738059057)]\n",
      "neighbors ids  [1, 2, 3, 0]\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.8339512944221497\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.8350105881690979\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.8294491767883301\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.7229872941970825\n",
      "neighbor best  [(2, 0.8350105881690979), (1, 0.8339512944221497), (3, 0.8294491767883301), (0, 0.7229872941970825)]\n",
      "name_file_neighbor_best  ucf101_3_ResNet152Transformer_set1_D1_best.hdf5\n",
      "u  0.2\n",
      "distance_ij  5669.348124077805\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:235: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sum_particle_tmp -= M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  10\n",
      "distance_ij  16507.36638778964\n",
      "tmp_lr  0.001\n",
      "u  0.2\n",
      "distance_ij  31629.245738059057\n",
      "tmp_lr  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6591/2509678.py:250: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 13  r1= 0.3542315338923121  r2= 0.5889895407133315  current acc= 0.8339512944221497  local best= 0.8339512944221497  neighbor index= 2  neighbor best= 0.8350105881690979\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 23:30:58.034867: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_1038134\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:617\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  0.0010000000474974513\n",
      " 399/1192 [=========>....................] - ETA: 10:09 - loss: 0.0452 - accuracy: 0.9909"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import keras\n",
    "import math\n",
    "\n",
    "#index of this pso\n",
    "pso_index = 1\n",
    "\n",
    "#number of neighbors (max=4)\n",
    "num_neighbors = 4\n",
    "#K coefficient\n",
    "M = 1\n",
    "u = 1\n",
    "\n",
    "tmp_acc = 0\n",
    "tmp_w = []\n",
    "pbest_acc = 0\n",
    "pbest_w = []\n",
    "\n",
    "#accelerator coefficient\n",
    "c1 = 0.5\n",
    "c2 = 0.5\n",
    "# w = 0.5\n",
    "\n",
    "r1 = 0\n",
    "r2 = 0\n",
    "\n",
    "results_stack_accuracy = []\n",
    "results_stack_val_accuracy = []\n",
    "results_stack_loss = []\n",
    "results_stack_val_loss = []\n",
    "\n",
    "#threshold\n",
    "# threshold = 0.97\n",
    "\n",
    "# #iteration control\n",
    "# i = 0\n",
    "# iter_max = 40\n",
    "\n",
    "warm_up = 0\n",
    "\n",
    "#    \n",
    "# time synchronize\n",
    "number_of_pso = 4\n",
    "training_start_flag = 1\n",
    "training_finish_flag = 0\n",
    "\n",
    "#set initial training flag to start\n",
    "set_training_flag(pso_index, training_start_flag)\n",
    "\n",
    "for index in range(warm_up, epochs): \n",
    "# while i < iter_max:\n",
    "    #start training \n",
    "    set_training_flag(pso_index, training_start_flag)\n",
    "    print(get_training_flag(pso_index))\n",
    "    \n",
    "    #save previous weight\n",
    "    model_mul.save_weights(savedfilename_pre) \n",
    "    \n",
    "    # result = model_mul.fit_generator(\n",
    "    #     generator = train_set, \n",
    "    #     steps_per_epoch = step_size_train,\n",
    "    #     validation_data = valid_set,\n",
    "    #     validation_steps = step_size_valid,\n",
    "    #     shuffle=True,\n",
    "    #     epochs=1,\n",
    "    #     callbacks=[checkpointer,tf.keras.callbacks.LearningRateScheduler(rand_scheduler)],\n",
    "    # #     callbacks=[csv_logger, checkpointer, earlystopping],\n",
    "    # #     callbacks=[tb, csv_logger, checkpointer, earlystopping],        \n",
    "    #     verbose=1) \n",
    "\n",
    "    result = model_mul.fit(\n",
    "        train,\n",
    "        validation_data=test,\n",
    "        verbose=1,   \n",
    "        epochs=1,\n",
    "        callbacks=[checkpointer,tf.keras.callbacks.LearningRateScheduler(rand_scheduler)],\n",
    "        # workers=2\n",
    "    ) \n",
    "    \n",
    "    #save weights every iteration\n",
    "#     model_mul.save_weights(savedfilename)\n",
    "    \n",
    "    tmp_acc = result.history.get('val_accuracy')[-1]\n",
    "    tmp_w = model_mul.get_weights()\n",
    "    tmp_lr = result.history.get('lr')[-1]\n",
    "    \n",
    "    #save current location in scoreboard\n",
    "    set_c_loc(pso_index,tmp_acc) \n",
    "    \n",
    "    if tmp_acc > pbest_acc:\n",
    "        pbest_acc = tmp_acc\n",
    "        pbest_w = tmp_w\n",
    "        #save person best location\n",
    "        set_pbest_loc(pso_index,pbest_acc)  \n",
    "        # save best model\n",
    "        model_mul.save_weights(savedfilename_best)        \n",
    "\n",
    "    #set training flag to finish\n",
    "    set_training_flag(pso_index, training_finish_flag)  \n",
    "    print(get_training_flag(pso_index))\n",
    "        \n",
    "    # check if all PSOs is ready (flag==1)\n",
    "    while(True):\n",
    "        tmp_flag = 0\n",
    "        for flg_i in range(number_of_pso):\n",
    "            print(\"flg_i\", flg_i, \"flag\", get_training_flag(flg_i))\n",
    "            if(get_training_flag(flg_i) == 1):\n",
    "                tmp_flag = 1\n",
    "        if(tmp_flag==1):\n",
    "            #waiting for 60s\n",
    "            print(\"\\n\")\n",
    "            for i in range(60,0,-1):\n",
    "                print(\"waiting for ....%2d\" %i, end=\"\\r\", flush=True)\n",
    "                time.sleep(1)    \n",
    "        else:\n",
    "            print(\"end of waiting\")\n",
    "            break     \n",
    "        \n",
    "    r1 = random.uniform(0,1)\n",
    "    r2 = random.uniform(0,1)\n",
    "#     r3 = random.uniform(0,1)    \n",
    "    \n",
    "    #-----------nearest neighbor best--------------\n",
    "    #get neighbor weights\n",
    "    #1\n",
    "    neighbor_c_acc_1, name_file_1 = get_c_loc(0)\n",
    "    neighbor_c_acc_2, name_file_2 = get_c_loc(1)\n",
    "    neighbor_c_acc_3, name_file_3 = get_c_loc(2)\n",
    "    neighbor_c_acc_4, name_file_4 = get_c_loc(3)\n",
    "    \n",
    "    #get pre loc\n",
    "    neighbor_pre_acc_1, name_pre_file_1 = get_pre_loc(0)\n",
    "    neighbor_pre_acc_2, name_pre_file_2 = get_pre_loc(1)\n",
    "    neighbor_pre_acc_3, name_pre_file_3 = get_pre_loc(2)\n",
    "    neighbor_pre_acc_4, name_pre_file_4 = get_pre_loc(3)  \n",
    "    \n",
    "    #clone model for weights change\n",
    "    model_clone = keras.models.clone_model(model_mul)\n",
    "    \n",
    "    model_clone.load_weights(os.path.join('checkpoints', name_file_1))\n",
    "    neighbor_w_1 = model_clone.get_weights() \n",
    "    model_clone.load_weights(os.path.join('checkpoints', name_file_2))\n",
    "    neighbor_w_2 = model_clone.get_weights()\n",
    "    model_clone.load_weights(os.path.join('checkpoints', name_file_3))\n",
    "    neighbor_w_3 = model_clone.get_weights()\n",
    "    model_clone.load_weights(os.path.join('checkpoints', name_file_4))\n",
    "    neighbor_w_4 = model_clone.get_weights()\n",
    "    \n",
    "    #clone model pre weights\n",
    "    model_clone.load_weights(os.path.join('checkpoints', name_pre_file_1))\n",
    "    neighbor_pre_w_1 = model_clone.get_weights()     \n",
    "    model_clone.load_weights(os.path.join('checkpoints', name_pre_file_2))\n",
    "    neighbor_pre_w_2 = model_clone.get_weights() \n",
    "    model_clone.load_weights(os.path.join('checkpoints', name_pre_file_3))\n",
    "    neighbor_pre_w_3 = model_clone.get_weights()    \n",
    "    model_clone.load_weights(os.path.join('checkpoints', name_pre_file_4))\n",
    "    neighbor_pre_w_4 = model_clone.get_weights()    \n",
    "    \n",
    "    distance_1 = find_distance(neighbor_w_2,neighbor_w_1)\n",
    "    distance_2 = find_distance(neighbor_w_2,neighbor_w_3)\n",
    "    distance_3 = find_distance(neighbor_w_2,neighbor_w_4)\n",
    "    \n",
    "    #find the closest neighbor\n",
    "    distances = list()\n",
    "    distances.append((0,distance_1))\n",
    "    distances.append((1,0))\n",
    "    distances.append((2,distance_2))\n",
    "    distances.append((3,distance_3))\n",
    "\n",
    "    print('distances unsorted', distances)\n",
    "    \n",
    "    distances.sort(key=lambda tup: tup[1])\n",
    "    print('distances ', distances)\n",
    "    \n",
    "    neighbors_idx = list()\n",
    "    for i in range(num_neighbors):\n",
    "        neighbors_idx.append(distances[i][0])        \n",
    "    \n",
    "    print('neighbors ids ', neighbors_idx)\n",
    "    \n",
    "    #get neighbor bests from the list\n",
    "    neighbor_bests = list()\n",
    "    #remove first element (self distance)\n",
    "#     neighbors_idx.pop(0)\n",
    "    \n",
    "    for i in range(len(neighbors_idx)):\n",
    "        neighbor_best_tmp, name_file_neighbor_best_tmp = get_pbest_loc(neighbors_idx[i])\n",
    "        neighbor_bests.append((neighbors_idx[i],neighbor_best_tmp))\n",
    "        print('neighbor_idx ', neighbors_idx[i])\n",
    "        print('neighbor_best_tmp ', neighbor_best_tmp)\n",
    "    \n",
    "    # keep unsorted list of neighbor\n",
    "    neighbor_tmp = deepcopy(neighbor_bests)\n",
    "    \n",
    "    # sort the list for maximum accuracy   \n",
    "    neighbor_bests.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    print('neighbor best ', neighbor_bests)\n",
    "    #\n",
    "    neighbor_best_value, name_file_neighbor_best = get_pbest_loc(neighbor_bests[0][0])\n",
    "    print('name_file_neighbor_best ', name_file_neighbor_best)\n",
    "    \n",
    "    model_clone.load_weights(os.path.join('checkpoints', name_file_neighbor_best))\n",
    "    neighbor_best_w = model_clone.get_weights()  \n",
    "    #---------- end nearest neighbor best ----------\n",
    "    \n",
    "    #---------- cucker -----------------------------\n",
    "    particle_w_i = neighbor_w_2\n",
    "    sum_particle_tmp = 0\n",
    "    \n",
    "    #remove the fist (self)\n",
    "    neighbor_tmp.pop(0)\n",
    "    \n",
    "    for j in range(len(neighbor_tmp)):\n",
    "        if neighbor_tmp[j][0]==0:\n",
    "            particle_w_j = neighbor_w_1\n",
    "            particle_w_pre_j = neighbor_pre_w_1\n",
    "            distance_ij = distance_1\n",
    "            u = 0.2\n",
    "        elif neighbor_tmp[j][0]==2:    \n",
    "            particle_w_j = neighbor_w_3\n",
    "            particle_w_pre_j = neighbor_pre_w_3\n",
    "            distance_ij = distance_2\n",
    "            u = 0.2\n",
    "        elif neighbor_tmp[j][0]==3:    \n",
    "            particle_w_j = neighbor_w_4\n",
    "            particle_w_pre_j = neighbor_pre_w_4\n",
    "            distance_ij = distance_3\n",
    "            u = 10\n",
    "            \n",
    "        print('u ', u)\n",
    "        print('distance_ij ', distance_ij)\n",
    "        print('tmp_lr ', tmp_lr)\n",
    "        #sum(K/(1+distance)*(particle_w_j-particle_w_i)\n",
    "        sum_particle_tmp -= M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j))\n",
    "#         sum_particle_tmp =  sum_particle_tmp \\\n",
    "#                             - M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j)) \\\n",
    "#                             + M*u*tmp_lr/(1+distance_ij)*np.array(particle_w_j)\n",
    "         \n",
    "    #---------- end cucker -------------------------\n",
    "\n",
    "    #update networks' weights\n",
    "    #     w = c1*r1*(np.array(pbest_w)-np.array(tmp_w))+c2*r2*(np.array(gbest_w)-np.array(tmp_w))\n",
    "    #     w = r1*np.array(pbest_w)+r2*np.array(tmp_w)+r3*np.array(gbest_w)\n",
    "    #     w = np.array(tmp_w)+tmp_lr*(c1*r1*(np.array(pbest_w)-np.array(tmp_w))+c2*r2*(np.array(gbest_w)-np.array(tmp_w)))\n",
    "#     final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n",
    "#     final_weight = np.array(tmp_w)+sum_particle_tmp\n",
    "\n",
    "#     final_weight = np.array(tmp_w)+sum_particle_tmp+c1*r1*(np.array(pbest_w)-np.array(tmp_w))+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n",
    "    final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n",
    "#     final_weight = np.array(tmp_w)+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n",
    "#     final_weight = np.array(tmp_w)+sum_particle_tmp\n",
    "    \n",
    "    model_mul.set_weights(final_weight)\n",
    "    \n",
    "    print('After ---> epoch=', index, ' r1=',r1, ' r2=',r2, ' current acc=', tmp_acc, ' local best=', pbest_acc, \n",
    "          ' neighbor index=', neighbor_bests[0][0], ' neighbor best=', neighbor_best_value)  \n",
    "    \n",
    "    results_stack_val_accuracy.append(result.history.get('val_accuracy')[-1])\n",
    "    results_stack_accuracy.append(result.history.get('accuracy')[-1])\n",
    "    results_stack_val_loss.append(result.history.get('val_loss')[-1])      \n",
    "    results_stack_loss.append(result.history.get('loss')[-1])\n",
    "    \n",
    "#     i = i + 1\n",
    "        \n",
    "print(results_stack_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b3d9b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8350105881690979\n"
     ]
    }
   ],
   "source": [
    "print(max(result.history.get('val_accuracy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b14400",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760b96d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae78b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e14f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !wget --no-check-certificate https://www.crcv.ucf.edu/data/UCF101/UCF101.rar\n",
    "# !wget --no-check-certificate https://www.crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05772bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !unrar e UCF101.rar data/\n",
    "# !unzip -qq UCF101TrainTestSplits-RecognitionTask.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb246c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move_videos(train_new, \"train\")\n",
    "# move_videos(test_new, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140807dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec4358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %mv UCF101.rar ucf101_v2\n",
    "# %mv UCF101TrainTestSplits-RecognitionTask.zip ucf101_v2\n",
    "# %mkdir ucf101_v2/set1\n",
    "# %mv train ucf101_v2/set1\n",
    "# %mv test ucf101_v2/set1\n",
    "# %mv data ucf101_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cdee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222c8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f32ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
