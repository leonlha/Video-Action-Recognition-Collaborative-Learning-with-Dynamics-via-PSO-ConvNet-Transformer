{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef3b7bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/datastorage/Phong/ucf101_v2/set3\n"
     ]
    }
   ],
   "source": [
    "cd /media/datastorage/Phong/ucf101_v2/set3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfee7320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 40\r\n",
      "drwxrwxr-x   2 bribeiro bribeiro 4096 set 26 11:44 \u001b[0m\u001b[01;34mcheckpoints\u001b[0m/\r\n",
      "drwxrwxr-x 103 bribeiro bribeiro 4096 fev  4  2018 \u001b[01;34mtest\u001b[0m/\r\n",
      "drwxrwxr-x 103 bribeiro bribeiro 4096 fev  4  2018 \u001b[01;34mtrain\u001b[0m/\r\n",
      "-rw-rw-r--   1 bribeiro bribeiro  665 fev  6  2018 ucf101_1_densenet201_set3.csv\r\n",
      "-rw-rw-r--   1 bribeiro bribeiro  751 fev  8  2018 ucf101_1_densenet201_set3_frame2.csv\r\n",
      "-rw-rw-r--   1 bribeiro bribeiro  643 fev  4  2018 ucf101_1_resnet152_set3.csv\r\n",
      "-rw-rw-r--   1 bribeiro bribeiro  727 jan 28  2018 ucf101_1_resnet152_set3_frame2.csv\r\n",
      "-rw-rw-r--   1 bribeiro bribeiro  798 fev  3  2018 ucf101_TransformerDenseNet201_set3.csv\r\n",
      "-rw-rw-r--   1 bribeiro bribeiro  832 fev  3  2018 ucf101_TransformerDenseNet201_set3_D2.csv\r\n",
      "-rw-rw-r--   1 bribeiro bribeiro  675 set 26 11:44 ucf101_TransformerResNet152_set3_D2.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fd9704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08010f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91296015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "d = {'id': [1, 2, 3, 4], 'pbest_value': [0, 0, 0, 0], 'pbest_file':['ucf101_1_ResNet152Transformer_set3_D2_best.hdf5',\n",
    "                                                        'ucf101_2_ResNet152Transformer_set3_D2_best.hdf5',\n",
    "                                                        'ucf101_3_ResNet152Transformer_set3_D2_best.hdf5',\n",
    "                                                        'ucf101_4_ResNet152Transformer_set3_D2_best.hdf5'], \n",
    "                         'c_value': [0, 0, 0, 0], 'c_file':['ucf101_1_ResNet152Transformer_set3_D2.hdf5',\n",
    "                                                             'ucf101_2_ResNet152Transformer_set3_D2.hdf5',\n",
    "                                                             'ucf101_3_ResNet152Transformer_set3_D2.hdf5',\n",
    "                                                             'ucf101_4_ResNet152Transformer_set3_D2.hdf5'], \n",
    "                         'pre_value': [0, 0, 0, 0], 'pre_file':['ucf101_1_ResNet152Transformer_set3_D2_pre.hdf5',\n",
    "                                                             'ucf101_2_ResNet152Transformer_set3_D2_pre.hdf5',\n",
    "                                                             'ucf101_3_ResNet152Transformer_set3_D2_pre.hdf5',\n",
    "                                                             'ucf101_4_ResNet152Transformer_set3_D2_pre.hdf5'],\n",
    "                         'training_flag':[0, 0, 0, 0]\n",
    "    }\n",
    "df = pandas.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea553d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a97babd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first instance only\n",
    "df.to_csv(os.path.join('ucf101_TransformerResNet152_set3_D2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9b7050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = os.path.join('ucf101_TransformerResNet152_set3_D2.csv')\n",
    "\n",
    "def synch_read_data(data_file=''):\n",
    "    while(True):\n",
    "        try:\n",
    "            df = pandas.read_csv(data_file, index_col=0)  \n",
    "            break                     \n",
    "        except:\n",
    "            #waiting for 10s\n",
    "            print(\"\\n\")\n",
    "            for i in range(10,0,-1):\n",
    "                print(\"re-read the file ....%2d\" %i, end=\"\\r\", flush=True)\n",
    "                time.sleep(1) \n",
    "    return df  \n",
    "\n",
    "def synch_write_data(df,data_file=''):\n",
    "    while(True):\n",
    "        try:\n",
    "            df.to_csv(data_file)  \n",
    "            break                     \n",
    "        except:\n",
    "            #waiting for 10s\n",
    "            print(\"\\n\")\n",
    "            for i in range(10,0,-1):\n",
    "                print(\"re-read the file ....%2d\" %i, end=\"\\r\", flush=True)\n",
    "                time.sleep(1) \n",
    "    return df  \n",
    "\n",
    "def get_pbest_loc(row=0):\n",
    "    df = synch_read_data(data_file)\n",
    "    row=df.loc[row]\n",
    "    pbest_value = row[1]\n",
    "    file_name = row[2]\n",
    "    return pbest_value, file_name\n",
    "\n",
    "def set_pbest_loc(row, pbest_value):\n",
    "    df = synch_read_data(data_file)\n",
    "    df.loc[row, 'pbest_value'] = pbest_value\n",
    "    synch_write_data(df,data_file)\n",
    "    \n",
    "def get_c_loc(row=0):\n",
    "    df = synch_read_data(data_file)\n",
    "    row=df.loc[row]\n",
    "    c_value = row[3]\n",
    "    file_name = row[4]\n",
    "    return c_value, file_name\n",
    "\n",
    "def set_c_loc(row, c_value):\n",
    "    df = synch_read_data(data_file)\n",
    "    df.loc[row, 'c_value'] = c_value\n",
    "    synch_write_data(df,data_file)   \n",
    "\n",
    "#    \n",
    "def get_pre_loc(row=0):\n",
    "    df = synch_read_data(data_file)\n",
    "    row=df.loc[row]\n",
    "    pre_value = row[5]\n",
    "    file_name = row[6]\n",
    "    return pre_value, file_name\n",
    "\n",
    "def set_pre_loc(row, pre_value):\n",
    "    df = synch_read_data(data_file)\n",
    "    df.loc[row, 'pre_value'] = pre_value\n",
    "    synch_write_data(df,data_file)\n",
    "    \n",
    "#training flag\n",
    "def get_training_flag(row=0):\n",
    "    df = synch_read_data(data_file)\n",
    "    row=df.loc[row]\n",
    "    training_flag = row[7]\n",
    "    return training_flag\n",
    "\n",
    "def set_training_flag(row, training_status):\n",
    "    df = synch_read_data(data_file)\n",
    "    df.loc[row, 'training_flag'] = training_status\n",
    "    synch_write_data(df,data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "518e1c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_distance(w1, w2):\n",
    "    sqr_distance = 0\n",
    "    \n",
    "    w_np_1 = np.array(w1)\n",
    "    w_fl_1 = w_np_1.flatten()\n",
    "    w_np_2 = np.array(w2)\n",
    "    w_fl_2 = w_np_2.flatten()\n",
    "    \n",
    "    for i in range(len(w_np_1)):\n",
    "        x1_fl = w_fl_1[i].flatten()\n",
    "        x2_fl = w_fl_2[i].flatten()\n",
    "\n",
    "        tmp_dis = 0 \n",
    "        for j in range(len(x1_fl)):\n",
    "            tmp_dis = tmp_dis + (x1_fl[j]-x2_fl[j])**2\n",
    "\n",
    "    #     print(tmp_dis)\n",
    "        sqr_distance = sqr_distance + tmp_dis\n",
    "\n",
    "    return sqr_distance**(1/2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f92ae7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "#Stop training on val_acc\n",
    "class EarlyStoppingByAccVal(Callback):\n",
    "    def __init__(self, monitor='val_acc', value=0.00001, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "\n",
    "        if current >= self.value:\n",
    "            if self.verbose > 0:\n",
    "                print(\"Epoch %05d: early stopping\" % epoch)\n",
    "            self.model.stop_training = True\n",
    "\n",
    "#Save large model using pickle formate instead of h5            \n",
    "class SaveCheckPoint(Callback):\n",
    "    def __init__(self, model, dest_folder):\n",
    "        super(Callback, self).__init__()\n",
    "        self.model = model\n",
    "        self.dest_folder = dest_folder\n",
    "        \n",
    "        #initiate\n",
    "        self.best_val_acc = 0\n",
    "        self.best_val_loss = sys.maxsize #get max value\n",
    "          \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_acc = logs['val_acc']\n",
    "        val_loss = logs['val_loss']\n",
    "\n",
    "        if val_acc > self.best_val_acc:\n",
    "            self.best_val_acc = val_acc\n",
    "            \n",
    "            # Save weights in pickle format instead of h5\n",
    "            print('\\nSaving val_acc %f at %s' %(self.best_val_acc, self.dest_folder))\n",
    "            weigh= self.model.get_weights()\n",
    "\n",
    "            #now, use pickle to save your model weights, instead of .h5\n",
    "            #for heavy model architectures, .h5 file is unsupported.\n",
    "            fpkl= open(self.dest_folder, 'wb') #Python 3\n",
    "            pickle.dump(weigh, fpkl, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "            fpkl.close()\n",
    "            \n",
    "#             model.save('tmp.h5')\n",
    "        elif val_acc == self.best_val_acc:\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss=val_loss\n",
    "                \n",
    "                # Save weights in pickle format instead of h5\n",
    "                print('\\nSaving val_acc %f at %s' %(self.best_val_acc, self.dest_folder))\n",
    "                weigh= self.model.get_weights()\n",
    "\n",
    "                #now, use pickle to save your model weights, instead of .h5\n",
    "                #for heavy model architectures, .h5 file is unsupported.\n",
    "                fpkl= open(self.dest_folder, 'wb') #Python 3\n",
    "                pickle.dump(weigh, fpkl, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "                fpkl.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ababf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import paths\n",
    "from tqdm import tqdm\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import shutil\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f54a147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open the .txt file which have names of training videos\n",
    "# f = open(\"ucfTrainTestlist/trainlist01.txt\", \"r\")\n",
    "# temp = f.read()\n",
    "# videos = temp.split('\\n')\n",
    "\n",
    "# # Create a dataframe having video names\n",
    "# train = pd.DataFrame()\n",
    "# train['video_name'] = videos\n",
    "# train = train[:-1]\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f4a77f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a0bf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open the .txt file which have names of test videos\n",
    "# with open(\"ucfTrainTestlist/testlist01.txt\", \"r\") as f:\n",
    "#     temp = f.read()\n",
    "# videos = temp.split(\"\\n\")\n",
    "\n",
    "# # Create a dataframe having video names\n",
    "# test = pd.DataFrame()\n",
    "# test[\"video_name\"] = videos\n",
    "# test = test[:-1]\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7629a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_tag(video_path):\n",
    "#     return video_path.split(\"/\")[0]\n",
    "\n",
    "# def separate_video_name(video_name):\n",
    "#     return video_name.split(\"/\")[1]\n",
    "\n",
    "# def rectify_video_name(video_name):\n",
    "#     return video_name.split(\" \")[0]\n",
    "\n",
    "# # def move_videos(df, output_dir):\n",
    "# #     if not os.path.exists(output_dir):\n",
    "# #         os.mkdir(output_dir)\n",
    "# #     for i in tqdm(range(df.shape[0])):\n",
    "# #         videoFile = df['video_name'][i].split(\"/\")[-1]\n",
    "# #         videoPath = os.path.join(\"data\", videoFile)\n",
    "# #         shutil.copy2(videoPath, output_dir)\n",
    "# #     print()\n",
    "# #     print(f\"Total videos: {len(os.listdir(output_dir))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1286dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[\"tag\"] = train[\"video_name\"].apply(extract_tag)\n",
    "# train[\"video_name\"] = train[\"video_name\"].apply(separate_video_name)\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e0e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[\"video_name\"] = train[\"video_name\"].apply(rectify_video_name)\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4449d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test[\"tag\"] = test[\"video_name\"].apply(extract_tag)\n",
    "# test[\"video_name\"] = test[\"video_name\"].apply(separate_video_name)\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98061a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 101\n",
    "# topNActs = train[\"tag\"].value_counts().nlargest(n).reset_index()[\"index\"].tolist()\n",
    "# train_new = train[train[\"tag\"].isin(topNActs)]\n",
    "# test_new = test[test[\"tag\"].isin(topNActs)]\n",
    "# train_new.shape, test_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c63961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_new = train_new.reset_index(drop=True)\n",
    "# test_new = test_new.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34b04ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def move_videos(df, output_dir):\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.mkdir(output_dir)\n",
    "#     for i in tqdm(range(df.shape[0])):\n",
    "#         videoFile = df['video_name'][i].split(\"/\")[-1]\n",
    "#         videoTag = df['tag'][i]\n",
    "#         videoPath = os.path.join(\"data\", videoFile)\n",
    "#         output_folder = os.path.join(output_dir, videoTag)\n",
    "#         if not os.path.exists(output_folder):\n",
    "#             os.mkdir(output_folder)\n",
    "#         shutil.copy2(videoPath, output_folder)\n",
    "#     print()\n",
    "#     print(f\"Total videos: {len(os.listdir(output_dir))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735e0280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move_videos(train_new, \"train\")\n",
    "# move_videos(test_new, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65b6e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VideoFrameGenerator - Simple Generator\n",
    "--------------------------------------\n",
    "A simple frame generator that takes distributed frames from\n",
    "videos. It is useful for videos that are scaled from frame 0 to end\n",
    "and that have no noise frames.\n",
    "\"\"\"\n",
    "\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "from math import floor\n",
    "from typing import Iterable, Optional\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import (ImageDataGenerator,\n",
    "                                                  img_to_array)\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "# from tensorflow.keras import backend as K\n",
    "# # Don't Show Warning Messages\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# import gc; gc.enable()\n",
    "\n",
    "log = logging.getLogger()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class VideoFrameGenerator(Sequence):  # pylint: disable=too-many-instance-attributes\n",
    "    \"\"\"\n",
    "    Create a generator that return batches of frames from video\n",
    "    # - rescale: float fraction to rescale pixel data (commonly 1/255.)\n",
    "    - nb_frames: int, number of frames to return for each sequence\n",
    "    - classes: list of str, classes to infer\n",
    "    - batch_size: int, batch size for each loop\n",
    "    - use_frame_cache: bool, use frame cache (may take a lot of memory for \\\n",
    "        large dataset)\n",
    "    - shape: tuple, target size of the frames\n",
    "    - shuffle: bool, randomize files\n",
    "    - transformation: ImageDataGenerator with transformations\n",
    "    - split: float, factor to split files and validation\n",
    "    - nb_channel: int, 1 or 3, to get grayscaled or RGB images\n",
    "    - glob_pattern: string, directory path with '{classname}' inside that \\\n",
    "        will be replaced by one of the class list\n",
    "    - use_header: bool, default to True to use video header to read the \\\n",
    "        frame count if possible\n",
    "    - seed: int, default to None, keep the seed value for split\n",
    "    You may use the \"classes\" property to retrieve the class list afterward.\n",
    "    The generator has that properties initialized:\n",
    "    - classes_count: number of classes that the generator manages\n",
    "    - files_count: number of video that the generator can provides\n",
    "    - classes: the given class list\n",
    "    - files: the full file list that the generator will use, this \\\n",
    "        is usefull if you want to remove some files that should not be \\\n",
    "        used by the generator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(  # pylint: disable=too-many-statements,too-many-locals,too-many-branches,too-many-arguments\n",
    "        self,\n",
    "        # rescale: float = 1 / 255.0,\n",
    "        nb_frames: int = 5,\n",
    "        classes: list = None,\n",
    "        batch_size: int = 16,\n",
    "        use_frame_cache: bool = False,\n",
    "        target_shape: tuple = (224, 224),\n",
    "        shuffle: bool = True,\n",
    "        transformation: Optional[ImageDataGenerator] = None,\n",
    "        split_test: float = None,\n",
    "        split_val: float = None,\n",
    "        nb_channel: int = 3,\n",
    "        glob_pattern: str = \"./videos/{classname}/*.avi\",\n",
    "        use_headers: bool = True,\n",
    "        seed=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        self.glob_pattern = glob_pattern\n",
    "\n",
    "        # should be only RGB or Grayscale\n",
    "        assert nb_channel in (1, 3)\n",
    "\n",
    "        if classes is None:\n",
    "            classes = self._discover_classes()\n",
    "\n",
    "        # we should have classes\n",
    "        if len(classes) == 0:\n",
    "            log.warn(\n",
    "                \"You didn't provide classes list or \"\n",
    "                \"we were not able to discover them from \"\n",
    "                \"your pattern.\\n\"\n",
    "                \"Please check if the path is OK, and if the glob \"\n",
    "                \"pattern is correct.\\n\"\n",
    "                \"See https://docs.python.org/3/library/glob.html\"\n",
    "            )\n",
    "\n",
    "        # shape size should be 2\n",
    "        assert len(target_shape) == 2\n",
    "\n",
    "        # split factor should be a propoer value\n",
    "        if split_val is not None:\n",
    "            assert 0.0 < split_val < 1.0\n",
    "\n",
    "        if split_test is not None:\n",
    "            assert 0.0 < split_test < 1.0\n",
    "\n",
    "        self.use_video_header = use_headers\n",
    "\n",
    "        # then we don't need None anymore\n",
    "        split_val = split_val if split_val is not None else 0.0\n",
    "        split_test = split_test if split_test is not None else 0.0\n",
    "\n",
    "        # be sure that classes are well ordered\n",
    "        classes.sort()\n",
    "\n",
    "        # self.rescale = rescale\n",
    "        self.classes = classes\n",
    "        self.batch_size = batch_size\n",
    "        self.nbframe = nb_frames\n",
    "        self.shuffle = shuffle\n",
    "        self.target_shape = target_shape\n",
    "        self.nb_channel = nb_channel\n",
    "        self.transformation = transformation\n",
    "        self.use_frame_cache = use_frame_cache\n",
    "\n",
    "        self._random_trans = []\n",
    "        self.__frame_cache = {}\n",
    "        self.files = []\n",
    "        self.validation = []\n",
    "        self.test = []\n",
    "\n",
    "        _validation_data = kwargs.get(\"_validation_data\", None)\n",
    "        _test_data = kwargs.get(\"_test_data\", None)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        if _validation_data is not None:\n",
    "            # we only need to set files here\n",
    "            self.files = _validation_data\n",
    "\n",
    "        elif _test_data is not None:\n",
    "            # we only need to set files here\n",
    "            self.files = _test_data\n",
    "        else:\n",
    "            self.__split_from_vals(\n",
    "                split_val, split_test, classes, shuffle, glob_pattern\n",
    "            )\n",
    "\n",
    "        # build indexes\n",
    "        self.files_count = len(self.files)\n",
    "        self.indexes = np.arange(self.files_count)\n",
    "        self.classes_count = len(classes)\n",
    "\n",
    "        # to initialize transformations and shuffle indices\n",
    "        if \"no_epoch_at_init\" not in kwargs:\n",
    "            self.on_epoch_end()\n",
    "\n",
    "        kind = \"train\"\n",
    "        if _validation_data is not None:\n",
    "            kind = \"validation\"\n",
    "        elif _test_data is not None:\n",
    "            kind = \"test\"\n",
    "\n",
    "        self._current = 0\n",
    "        self._framecounters = {}\n",
    "        print(\n",
    "            \"Total data: %d classes for %d files for %s\"\n",
    "            % (self.classes_count, self.files_count, kind)\n",
    "        )\n",
    "\n",
    "    def count_frames(self, cap, name, force_no_headers=False):\n",
    "        \"\"\"Count number of frame for video\n",
    "        if it's not possible with headers\"\"\"\n",
    "        if not force_no_headers and name in self._framecounters:\n",
    "            return self._framecounters[name]\n",
    "\n",
    "        total = cap.get(cv.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "        if force_no_headers or total < 0:\n",
    "            # headers not ok\n",
    "            total = 0\n",
    "            # TODO: we're unable to use CAP_PROP_POS_FRAME here\n",
    "            # so we open a new capture to not change the\n",
    "            # pointer position of \"cap\"\n",
    "            capture = cv.VideoCapture(name)\n",
    "            while True:\n",
    "                grabbed, _ = capture.read()\n",
    "                if not grabbed:\n",
    "                    # rewind and stop\n",
    "                    break\n",
    "                total += 1\n",
    "\n",
    "        # keep the result\n",
    "        self._framecounters[name] = total\n",
    "\n",
    "        return total\n",
    "\n",
    "    def __split_from_vals(self, split_val, split_test, classes, shuffle, glob_pattern):\n",
    "        \"\"\" Split validation and test set \"\"\"\n",
    "\n",
    "        if split_val == 0 or split_test == 0:\n",
    "            # no splitting, do the simplest thing\n",
    "            for cls in classes:\n",
    "                self.files += glob.glob(glob_pattern.format(classname=cls))\n",
    "            return\n",
    "\n",
    "        # else, there is some split to do\n",
    "        for cls in classes:\n",
    "            files = glob.glob(glob_pattern.format(classname=cls))\n",
    "            nbval = 0\n",
    "            nbtest = 0\n",
    "            info = []\n",
    "\n",
    "            # generate validation and test indexes\n",
    "            indexes = np.arange(len(files))\n",
    "\n",
    "            if shuffle:\n",
    "                np.random.shuffle(indexes)\n",
    "\n",
    "            nbtrain = 0\n",
    "            if 0.0 < split_val < 1.0:\n",
    "                nbval = int(split_val * len(files))\n",
    "                nbtrain = len(files) - nbval\n",
    "\n",
    "                # get some sample for validation_data\n",
    "                val = np.random.permutation(indexes)[:nbval]\n",
    "\n",
    "                # remove validation from train\n",
    "                indexes = np.array([i for i in indexes if i not in val])\n",
    "                self.validation += [files[i] for i in val]\n",
    "                info.append(\"validation count: %d\" % nbval)\n",
    "\n",
    "            if 0.0 < split_test < 1.0:\n",
    "                nbtest = int(split_test * nbtrain)\n",
    "                nbtrain = len(files) - nbval - nbtest\n",
    "\n",
    "                # get some sample for test_data\n",
    "                val_test = np.random.permutation(indexes)[:nbtest]\n",
    "\n",
    "                # remove test from train\n",
    "                indexes = np.array([i for i in indexes if i not in val_test])\n",
    "                self.test += [files[i] for i in val_test]\n",
    "                info.append(\"test count: %d\" % nbtest)\n",
    "\n",
    "            # and now, make the file list\n",
    "            self.files += [files[i] for i in indexes]\n",
    "            print(\"class %s, %s, train count: %d\" % (cls, \", \".join(info), nbtrain))\n",
    "\n",
    "    def _discover_classes(self):\n",
    "        pattern = os.path.realpath(self.glob_pattern)\n",
    "        pattern = re.escape(pattern)\n",
    "        pattern = pattern.replace(\"\\\\{classname\\\\}\", \"(.*?)\")\n",
    "        pattern = pattern.replace(\"\\\\*\", \".*\")\n",
    "\n",
    "        files = glob.glob(self.glob_pattern.replace(\"{classname}\", \"*\"))\n",
    "        classes = set()\n",
    "        for filename in files:\n",
    "            filename = os.path.realpath(filename)\n",
    "            classname = re.findall(pattern, filename)[0]\n",
    "            classes.add(classname)\n",
    "\n",
    "        return list(classes)\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\" Return next element\"\"\"\n",
    "        elem = self[self._current]\n",
    "        self._current += 1\n",
    "        if self._current == len(self):\n",
    "            self._current = 0\n",
    "            self.on_epoch_end()\n",
    "\n",
    "        return elem\n",
    "\n",
    "    # def get_validation_generator(self):\n",
    "    #     \"\"\" Return the validation generator if you've provided split factor \"\"\"\n",
    "    #     return self.__class__(\n",
    "    #         nb_frames=self.nbframe,\n",
    "    #         nb_channel=self.nb_channel,\n",
    "    #         target_shape=self.target_shape,\n",
    "    #         classes=self.classes,\n",
    "    #         batch_size=self.batch_size,\n",
    "    #         shuffle=self.shuffle,\n",
    "    #         # rescale=self.rescale,\n",
    "    #         glob_pattern=self.glob_pattern,\n",
    "    #         use_headers=self.use_video_header,\n",
    "    #         _validation_data=self.validation,\n",
    "    #     )\n",
    "\n",
    "    # def get_test_generator(self):\n",
    "    #     \"\"\" Return the validation generator if you've provided split factor \"\"\"\n",
    "    #     return self.__class__(\n",
    "    #         nb_frames=self.nbframe,\n",
    "    #         nb_channel=self.nb_channel,\n",
    "    #         target_shape=self.target_shape,\n",
    "    #         classes=self.classes,\n",
    "    #         batch_size=self.batch_size,\n",
    "    #         shuffle=self.shuffle,\n",
    "    #         # rescale=self.rescale,\n",
    "    #         glob_pattern=self.glob_pattern,\n",
    "    #         use_headers=self.use_video_header,\n",
    "    #         _test_data=self.test,\n",
    "    #     )\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\" Called by Keras after each epoch \"\"\"\n",
    "\n",
    "        if self.transformation is not None:\n",
    "            self._random_trans = []\n",
    "            for _ in range(self.files_count):\n",
    "                self._random_trans.append(\n",
    "                    self.transformation.get_random_transform(self.target_shape)\n",
    "                )\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)          \n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.files_count / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        classes = self.classes\n",
    "        shape = self.target_shape\n",
    "        nbframe = self.nbframe\n",
    "\n",
    "        labels = []\n",
    "        images = []\n",
    "        \n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "\n",
    "        transformation = None\n",
    "\n",
    "        for i in indexes:\n",
    "\n",
    "            video = self.files[i]\n",
    "            classname = self._get_classname(video)\n",
    "\n",
    "            # create a label array and set 1 to the right column\n",
    "            label = np.zeros(len(classes))\n",
    "            col = classes.index(classname)\n",
    "            label[col] = 1.0\n",
    "\n",
    "#             if video not in self.__frame_cache:\n",
    "#                 frames = self._get_frames(\n",
    "#                     video, nbframe, shape, force_no_headers=not self.use_video_header\n",
    "#                 )\n",
    "#                 if frames is None:\n",
    "#                     # avoid failure, nevermind that video...\n",
    "#                     continue\n",
    "\n",
    "#                 # add to cache\n",
    "#                 if self.use_frame_cache:\n",
    "#                     self.__frame_cache[video] = frames\n",
    "\n",
    "#             else:\n",
    "#                 frames = self.__frame_cache[video]\n",
    "            frames = self._get_frames(\n",
    "                    video, nbframe, shape, force_no_headers=not self.use_video_header\n",
    "                )\n",
    "\n",
    "            # apply transformation\n",
    "            # if provided\n",
    "            if self.transformation is not None:\n",
    "                transformation = self._random_trans[i]\n",
    "                frames = [\n",
    "                    self.transformation.apply_transform(frame, transformation)\n",
    "                    if transformation is not None\n",
    "                    else frame\n",
    "                    for frame in frames\n",
    "                ]\n",
    "\n",
    "            # add the sequence in batch\n",
    "            images.append(frames)\n",
    "            labels.append(label)\n",
    "\n",
    "        return np.array(images), np.array(labels)\n",
    "\n",
    "    def _get_classname(self, video: str) -> str:\n",
    "        \"\"\" Find classname from video filename following the pattern \"\"\"\n",
    "\n",
    "        # work with real path\n",
    "        video = os.path.realpath(video)\n",
    "        pattern = os.path.realpath(self.glob_pattern)\n",
    "\n",
    "        # remove special regexp chars\n",
    "        pattern = re.escape(pattern)\n",
    "\n",
    "        # get back \"*\" to make it \".*\" in regexp\n",
    "        pattern = pattern.replace(\"\\\\*\", \".*\")\n",
    "\n",
    "        # use {classname} as a capture\n",
    "        pattern = pattern.replace(\"\\\\{classname\\\\}\", \"(.*?)\")\n",
    "\n",
    "        # and find all occurence\n",
    "        classname = re.findall(pattern, video)[0]\n",
    "        return classname\n",
    "\n",
    "    def _get_frames(\n",
    "        self, video, nbframe, shape, force_no_headers=False\n",
    "    ) -> Optional[Iterable]:\n",
    "        cap = cv.VideoCapture(video)\n",
    "        total_frames = self.count_frames(cap, video, force_no_headers)\n",
    "        orig_total = total_frames\n",
    "\n",
    "        # if total_frames % 2 != 0:\n",
    "        #     total_frames += 1\n",
    "\n",
    "        frame_step = floor(total_frames / (nbframe - 1))\n",
    "        # print('frame step = ', frame_step)\n",
    "        # TODO: fix that, a tiny video can have a frame_step that is\n",
    "        # under 1\n",
    "        frame_step = max(1, frame_step)\n",
    "        frames = []\n",
    "        frame_i = 0\n",
    "\n",
    "        # while True:\n",
    "        #     grabbed, frame = cap.read()\n",
    "        #     if not grabbed:\n",
    "        #         break\n",
    "\n",
    "        #     # ifixit: increase frame index\n",
    "        #     frame_i += 1\n",
    "        for index in range(nbframe):\n",
    "            # print('index=', index)\n",
    "            frame_pos = index*(frame_step-1)\n",
    "            # print('frame pos=', frame_pos)\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_pos)\n",
    "            grabbed, frame = cap.read()\n",
    "            if not grabbed:\n",
    "                break\n",
    "\n",
    "            frame_i = frame_pos\n",
    "            # print('frame_i=',frame_i)\n",
    "            self.__add_and_convert_frame(\n",
    "                frame, frame_i, frames, orig_total, shape, frame_step\n",
    "            )\n",
    "\n",
    "            if len(frames) == nbframe:\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if not force_no_headers and len(frames) != nbframe:\n",
    "            # There is a problem here\n",
    "            # That means that frame count in header is wrong or broken,\n",
    "            # so we need to force the full read of video to get the right\n",
    "            # frame counter\n",
    "            return self._get_frames(video, nbframe, shape, force_no_headers=True)\n",
    "\n",
    "        if force_no_headers and len(frames) != nbframe:\n",
    "            # and if we really couldn't find the real frame counter\n",
    "            # so we return None. Sorry, nothing can be done...\n",
    "            log.error(\n",
    "                f\"Frame count is not OK for video {video}, \"\n",
    "                f\"{total_frames} total, {len(frames)} extracted\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        return np.array(frames)\n",
    "\n",
    "    def __add_and_convert_frame(  # pylint: disable=too-many-arguments\n",
    "        self, frame, frame_i, frames, orig_total, shape, frame_step\n",
    "    ):\n",
    "        #frame_i += 1\n",
    "        # if frame_i in (1, orig_total) or frame_i % frame_step == 0:\n",
    "        # crop center\n",
    "        frame = self.__crop_center_square(frame)\n",
    "        # resize\n",
    "        frame = cv.resize(frame, shape)\n",
    "\n",
    "        # use RGB or Grayscale ?\n",
    "        frame = (\n",
    "            cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "            if self.nb_channel == 3\n",
    "            else cv.cvtColor(frame, cv.COLOR_RGB2GRAY)\n",
    "        )\n",
    "\n",
    "        # to np\n",
    "        frame = img_to_array(frame)# * self.rescale\n",
    "\n",
    "        # keep frame\n",
    "        # print('append frame at frame_i= ', frame_i)\n",
    "        frames.append(frame)\n",
    "\n",
    "    def __crop_center_square(\n",
    "        self, frame\n",
    "    ):\n",
    "        y, x = frame.shape[0:2]\n",
    "        min_dim = min(y, x)\n",
    "        start_x = (x // 2) - (min_dim // 2)\n",
    "        start_y = (y // 2) - (min_dim // 2)\n",
    "        return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad6235af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "Total data: 101 classes for 9624 files for train\n",
      "Total data: 101 classes for 3696 files for train\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "# from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "# from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "# from tensorflow.keras.applications.densenet import preprocess_input\n",
    "\n",
    "\n",
    "# from keras_video import VideoFrameGenerator\n",
    "# use sub directories names as classes\n",
    "classes = [i.split(os.path.sep)[1] for i in glob.glob('train/*')]\n",
    "classes.sort()\n",
    "print(len(classes))\n",
    "\n",
    "# some global params\n",
    "SIZE = (224, 224)\n",
    "CHANNELS = 3\n",
    "NBFRAME = 4 #5\n",
    "BS = 8\n",
    "#\n",
    "MAX_SEQ_LENGTH = NBFRAME#override max sequence length\n",
    "NUM_FEATURES = 2048#1920\n",
    "#\n",
    "INSHAPE=(NBFRAME,) + SIZE + (CHANNELS,) # (5, 112, 112, 3)\n",
    "# pattern to get videos and classes\n",
    "glob_train_pattern='train/{classname}/*'\n",
    "glob_test_pattern='test/{classname}/*'\n",
    "# for data augmentation\n",
    "data_train_aug = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    zoom_range=.1,\n",
    "    # horizontal_flip=True,\n",
    "    rotation_range=8,\n",
    "    width_shift_range=.2,\n",
    "    height_shift_range=.2,\n",
    "    preprocessing_function=preprocess_input,\n",
    "    )\n",
    "\n",
    "data_test_aug = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    )\n",
    "# Create video frame generator\n",
    "train = VideoFrameGenerator(\n",
    "    classes=classes, \n",
    "    glob_pattern=glob_train_pattern,\n",
    "    nb_frames=NBFRAME,\n",
    "    # split=.33, \n",
    "    shuffle=True,\n",
    "    batch_size=BS,\n",
    "    target_shape=SIZE,\n",
    "    nb_channel=CHANNELS,\n",
    "    transformation=data_train_aug,\n",
    "    use_frame_cache=True)\n",
    "\n",
    "# Create video frame generator\n",
    "test = VideoFrameGenerator(\n",
    "    classes=classes, \n",
    "    glob_pattern=glob_test_pattern,\n",
    "    nb_frames=NBFRAME,\n",
    "    # split=.33, \n",
    "    shuffle=False,\n",
    "    batch_size=BS,\n",
    "    target_shape=SIZE,\n",
    "    nb_channel=CHANNELS,\n",
    "    transformation=data_test_aug,\n",
    "    use_frame_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a0cea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, \\\n",
    "    MaxPool2D, GlobalMaxPool2D\n",
    "from tensorflow.keras.models import Model\n",
    "# from tf.keras.applications.mobilenet import preprocess_input\n",
    "\n",
    "\n",
    "def build_convnet(shape=(224, 224, 3)):\n",
    "    f1_base = tf.keras.applications.ResNet152(weights='imagenet', include_top=False, input_shape=shape)\n",
    "    # f1_base = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=shape)\n",
    "    # f1_base = tf.keras.applications.MobileNet(weights='imagenet', include_top=False, input_shape=shape)\n",
    "#     f1_base = tf.keras.applications.DenseNet201(weights='imagenet', include_top=False, input_shape=shape)  \n",
    "    f1_x = f1_base.output\n",
    "\n",
    "    # #frozen layers    \n",
    "    # for layer in f1_base.layers:\n",
    "    #     layer.trainable = False  \n",
    "\n",
    "    f1_x = tf.keras.layers.GlobalAveragePooling2D()(f1_x)\n",
    "\n",
    "    model_1 = Model(inputs=[f1_base.input],outputs=[f1_x])        \n",
    "\n",
    "    return model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff68b8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "\n",
    "class PositionalEmbedding(tensorflow.keras.layers.Layer):\n",
    "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.position_embeddings = tensorflow.keras.layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
    "        length = tf.shape(inputs)[1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return inputs + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
    "        return mask\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'sequence_length': self.sequence_length,\n",
    "            'output_dim': self.output_dim,\n",
    "        })\n",
    "        return config    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7aa6d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(tensorflow.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = tensorflow.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
    "        )\n",
    "        self.dense_proj = tensorflow.keras.Sequential(\n",
    "            [tensorflow.keras.layers.Dense(dense_dim, activation=tf.nn.gelu), tensorflow.keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = tensorflow.keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = tensorflow.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'dense_dim': self.dense_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "        })\n",
    "        return config     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3261b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TimeDistributed, GRU, Dense, Dropout, GaussianNoise, GlobalMaxPooling1D\n",
    "\n",
    "def adv_action_model(shape=(4, 224, 224, 3), nbout=3):\n",
    "    sequence_length = MAX_SEQ_LENGTH\n",
    "    embed_dim = NUM_FEATURES\n",
    "    dense_dim = 64\n",
    "    num_heads = 4\n",
    "    classes = 101 #len(label_processor.get_vocabulary())\n",
    "    \n",
    "    # Create our convnet with (112, 112, 3) input shape\n",
    "    convnet = build_convnet(shape[1:])\n",
    "\n",
    "    # for layer in convnet.layers:\n",
    "    #     print(layer.name, ': ', layer.trainable)   \n",
    "    \n",
    "    # then create our final model\n",
    "    model = tf.keras.Sequential()\n",
    "    # add the convnet with (5, 112, 112, 3) shape\n",
    "    model.add(TimeDistributed(convnet, input_shape=shape))\n",
    "#     # here, you can also use GRU or LSTM\n",
    "#     model.add(GRU(2048))\n",
    "    model.add(PositionalEmbedding(\n",
    "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
    "    ))\n",
    "    model.add(TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\"))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    #Regularization with noise\n",
    "    model.add(GaussianNoise(0.1))\n",
    "    # and finally, we make a decision network\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(.4))\n",
    "    model.add(Dense(nbout, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4ce3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSHAPE=(NBFRAME,) + SIZE + (CHANNELS,) # (5, 112, 112, 3)\n",
    "\n",
    "# print(INSHAPE)\n",
    "# print(len(classes))\n",
    "# model = adv_action_model(INSHAPE, len(classes))\n",
    "# # optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "# optimizer = tf.keras.optimizers.SGD(0.01)\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer,\n",
    "#     'categorical_crossentropy',\n",
    "#     metrics=['acc'],\n",
    "#     run_eagerly=True\n",
    "# )\n",
    "\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c72b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "\n",
    "# EPOCHS=60\n",
    "# # create a \"chkp\" directory before to run that\n",
    "# # because ModelCheckpoint will write models inside\n",
    "# callbacks = [\n",
    "#     # tf.keras.callbacks.ReduceLROnPlateau(verbose=1),\n",
    "#     # tf.keras.callbacks.ModelCheckpoint(\n",
    "#     #     'chkp/weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "#     #     verbose=1),\n",
    "# ]\n",
    "# model.fit(\n",
    "#     train,\n",
    "#     validation_data=test,\n",
    "#     verbose=1,\n",
    "    \n",
    "#     epochs=EPOCHS,\n",
    "# #     callbacks=callbacks,\n",
    "#     # workers=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734b67fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2f9502b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of GPUs: 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 11:47:08.550654: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-26 11:47:09.258306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10394 MB memory:  -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "import time, os\n",
    "from math import ceil\n",
    "import random\n",
    "\n",
    "model_txt = 'st'\n",
    "# Helper: Save the model.\n",
    "savedfilename = os.path.join('checkpoints', 'ucf101_4_ResNet152Transformer_set3_D2.hdf5')\n",
    "savedfilename_best = os.path.join('checkpoints', 'ucf101_4_ResNet152Transformer_set3_D2_best.hdf5')\n",
    "savedfilename_pre = os.path.join('checkpoints', 'ucf101_4_ResNet152Transformer_set3_D2_pre.hdf5')\n",
    "\n",
    "checkpointer = ModelCheckpoint(savedfilename,\n",
    "                          monitor='val_accuracy', verbose=1, \n",
    "                          save_best_only=False, mode='max',save_weights_only=True)########\n",
    "\n",
    "# Helper: TensorBoard\n",
    "tb = TensorBoard(log_dir=os.path.join('svhn_output', 'logs', model_txt))\n",
    "\n",
    "# Helper: Save results.\n",
    "timestamp = time.time()\n",
    "csv_logger = CSVLogger(os.path.join('svhn_output', 'logs', model_txt + '-' + 'training-' + \\\n",
    "    str(timestamp) + '.log'))\n",
    "\n",
    "earlystopping = EarlyStoppingByAccVal(monitor='val_accuracy', value=0.9900, verbose=1)\n",
    "\n",
    "def rand_scheduler(epoch, lr):\n",
    "    rnd_lr = 10**(random.uniform(np.log10((1e-5)),np.log10((1e-1))))\n",
    "#     if epoch < 30:\n",
    "#         rnd_lr = 1e-2\n",
    "#     else:    \n",
    "#         rnd_lr = 1e-3\n",
    "#     rnd_lr = lr\n",
    "    print('random lr = ', rnd_lr)\n",
    "    return rnd_lr\n",
    "\n",
    "epochs = 20##!!!\n",
    "lr = 1e-3\n",
    "# decay = lr/epochs\n",
    "# optimizer = Adam(lr=lr, decay=decay)\n",
    "# optimizer = Adam(lr=lr)\n",
    "optimizer = SGD(learning_rate=lr)\n",
    "\n",
    "# train on multiple-gpus\n",
    "# Create a MirroredStrategy.\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of GPUs: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Open a strategy scope.\n",
    "with strategy.scope():\n",
    "    # Everything that creates variables should be under the strategy scope.\n",
    "    # In general this is only model construction & `compile()`.\n",
    "    model_mul = adv_action_model(INSHAPE, len(classes))\n",
    "    model_mul.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    # save initial model\n",
    "    model_mul.save_weights(savedfilename)\n",
    "    model_mul.save_weights(savedfilename_best)\n",
    "    model_mul.save_weights(savedfilename_pre)\n",
    "    \n",
    "# step_size_train=ceil(train_set.n/train_set.batch_size)\n",
    "# step_size_valid=ceil(valid_set.n/valid_set.batch_size)\n",
    "# step_size_test=ceil(testing_set.n//testing_set.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b3d9b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 11:47:31.710857: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_44302\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\020FlatMapDataset:1\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  0.08441858250035127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 11:48:35.356531: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n",
      "2022-09-26 11:48:35.722078: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-09-26 11:48:35.723055: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-09-26 11:48:35.723113: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2022-09-26 11:48:35.725573: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-09-26 11:48:35.725658: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1203/1203 [==============================] - ETA: 0s - loss: 4.9346 - accuracy: 0.0094"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 12:01:45.841501: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_111193\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021FlatMapDataset:23\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_4_ResNet152Transformer_set3_D2.hdf5\n",
      "1203/1203 [==============================] - 1048s 817ms/step - loss: 4.9346 - accuracy: 0.0094 - val_loss: 4.6091 - val_accuracy: 0.0116 - lr: 0.0844\n",
      "0\n",
      "flg_i 0 flag 0\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_26977/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 331810.3254817719), (1, 335794.31386612623), (2, 337125.5432491579), (3, 0)]\n",
      "distances  [(3, 0), (0, 331810.3254817719), (1, 335794.31386612623), (2, 337125.5432491579)]\n",
      "neighbors ids  [3, 0, 1, 2]\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.0116341989487409\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.574945867061615\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.6060606241226196\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.0679112523794174\n",
      "neighbor best  [(1, 0.6060606241226196), (0, 0.574945867061615), (2, 0.0679112523794174), (3, 0.0116341989487409)]\n",
      "name_file_neighbor_best  ucf101_2_ResNet152Transformer_set3_D2_best.hdf5\n",
      "u  0.2\n",
      "distance_ij  331810.3254817719\n",
      "tmp_lr  0.08441858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/4153759187.py:237: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  - M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j)) \\\n",
      "/tmp/ipykernel_26977/4153759187.py:238: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  + M*u*tmp_lr/(1+distance_ij)*np.array(particle_w_j)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  0.2\n",
      "distance_ij  335794.31386612623\n",
      "tmp_lr  0.08441858\n",
      "u  0.2\n",
      "distance_ij  337125.5432491579\n",
      "tmp_lr  0.08441858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/4153759187.py:251: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 0  r1= 0.2225130622951197  r2= 0.9894829061099506  current acc= 0.01163419894874096  local best= 0.01163419894874096  neighbor index= 1  neighbor best= 0.6060606241226196\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 12:15:50.183485: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_179220\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021FlatMapDataset:45\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  0.03397595334733698\n",
      "1203/1203 [==============================] - ETA: 0s - loss: 4.6115 - accuracy: 0.0107"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 12:29:45.332557: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_185431\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021FlatMapDataset:67\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_4_ResNet152Transformer_set3_D2.hdf5\n",
      "1203/1203 [==============================] - 992s 824ms/step - loss: 4.6115 - accuracy: 0.0107 - val_loss: 4.6092 - val_accuracy: 0.0116 - lr: 0.0340\n",
      "0\n",
      "flg_i 0 flag 1\n",
      "flg_i 1 flag 1\n",
      "flg_i 2 flag 1\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 1\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_26977/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 85632.69967751227), (1, 87265.64445621769), (2, 89145.39515341957), (3, 0)]\n",
      "distances  [(3, 0), (0, 85632.69967751227), (1, 87265.64445621769), (2, 89145.39515341957)]\n",
      "neighbors ids  [3, 0, 1, 2]\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.0116341989487409\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.6279761791229248\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.7364718317985535\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.1826298683881759\n",
      "neighbor best  [(1, 0.7364718317985535), (0, 0.6279761791229248), (2, 0.1826298683881759), (3, 0.0116341989487409)]\n",
      "name_file_neighbor_best  ucf101_2_ResNet152Transformer_set3_D2_best.hdf5\n",
      "u  0.2\n",
      "distance_ij  85632.69967751227\n",
      "tmp_lr  0.033975955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/4153759187.py:237: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  - M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j)) \\\n",
      "/tmp/ipykernel_26977/4153759187.py:238: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  + M*u*tmp_lr/(1+distance_ij)*np.array(particle_w_j)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  0.2\n",
      "distance_ij  87265.64445621769\n",
      "tmp_lr  0.033975955\n",
      "u  0.2\n",
      "distance_ij  89145.39515341957\n",
      "tmp_lr  0.033975955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/4153759187.py:251: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 1  r1= 0.5182688277174873  r2= 0.3875798504548138  current acc= 0.01163419894874096  local best= 0.01163419894874096  neighbor index= 1  neighbor best= 0.7364718317985535\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 12:45:17.286023: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_242357\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021FlatMapDataset:89\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  0.0001282957864677086\n",
      "1203/1203 [==============================] - ETA: 0s - loss: 4.5993 - accuracy: 0.0208"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 12:59:20.068426: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_248568\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:111\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_4_ResNet152Transformer_set3_D2.hdf5\n",
      "1203/1203 [==============================] - 1007s 837ms/step - loss: 4.5993 - accuracy: 0.0208 - val_loss: 4.5938 - val_accuracy: 0.0273 - lr: 1.2830e-04\n",
      "0\n",
      "flg_i 0 flag 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_26977/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 54567.65970767143), (1, 56091.471695673004), (2, 57853.148584578965), (3, 0)]\n",
      "distances  [(3, 0), (0, 54567.65970767143), (1, 56091.471695673004), (2, 57853.148584578965)]\n",
      "neighbors ids  [3, 0, 1, 2]\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.0273268390446901\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.761904776096344\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.7524350881576538\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.2838203608989715\n",
      "neighbor best  [(0, 0.761904776096344), (1, 0.7524350881576538), (2, 0.2838203608989715), (3, 0.0273268390446901)]\n",
      "name_file_neighbor_best  ucf101_1_ResNet152Transformer_set3_D2_best.hdf5\n",
      "u  0.2\n",
      "distance_ij  54567.65970767143\n",
      "tmp_lr  0.00012829578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/4153759187.py:237: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  - M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j)) \\\n",
      "/tmp/ipykernel_26977/4153759187.py:238: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  + M*u*tmp_lr/(1+distance_ij)*np.array(particle_w_j)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  0.2\n",
      "distance_ij  56091.471695673004\n",
      "tmp_lr  0.00012829578\n",
      "u  0.2\n",
      "distance_ij  57853.148584578965\n",
      "tmp_lr  0.00012829578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/4153759187.py:251: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 2  r1= 0.5888043610018677  r2= 0.5431566137652329  current acc= 0.027326839044690132  local best= 0.027326839044690132  neighbor index= 0  neighbor best= 0.761904776096344\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 13:14:09.431781: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_308967\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:133\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  6.542268792286873e-05\n",
      "1203/1203 [==============================] - ETA: 0s - loss: 4.2194 - accuracy: 0.1514"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 13:28:14.249238: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_315178\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:155\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_4_ResNet152Transformer_set3_D2.hdf5\n",
      "1203/1203 [==============================] - 998s 829ms/step - loss: 4.2194 - accuracy: 0.1514 - val_loss: 4.0299 - val_accuracy: 0.2722 - lr: 6.5423e-05\n",
      "0\n",
      "flg_i 0 flag 0\n",
      "flg_i 1 flag 1\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_26977/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 30692.294452946924), (1, 30851.84605721108), (2, 32734.940199961766), (3, 0)]\n",
      "distances  [(3, 0), (0, 30692.294452946924), (1, 30851.84605721108), (2, 32734.940199961766)]\n",
      "neighbors ids  [3, 0, 1, 2]\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.2721861600875854\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.761904776096344\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.7835497856140137\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.4061147272586822\n",
      "neighbor best  [(1, 0.7835497856140137), (0, 0.761904776096344), (2, 0.4061147272586822), (3, 0.2721861600875854)]\n",
      "name_file_neighbor_best  ucf101_2_ResNet152Transformer_set3_D2_best.hdf5\n",
      "u  0.2\n",
      "distance_ij  30692.294452946924\n",
      "tmp_lr  6.542269e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/4153759187.py:237: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  - M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j)) \\\n",
      "/tmp/ipykernel_26977/4153759187.py:238: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  + M*u*tmp_lr/(1+distance_ij)*np.array(particle_w_j)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  0.2\n",
      "distance_ij  30851.84605721108\n",
      "tmp_lr  6.542269e-05\n",
      "u  0.2\n",
      "distance_ij  32734.940199961766\n",
      "tmp_lr  6.542269e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/4153759187.py:251: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 3  r1= 0.9420250169796319  r2= 0.039053781293733714  current acc= 0.27218616008758545  local best= 0.27218616008758545  neighbor index= 1  neighbor best= 0.7835497856140137\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 13:42:48.116456: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_375577\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:177\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  0.00010331846649461728\n",
      "1203/1203 [==============================] - ETA: 0s - loss: 3.6948 - accuracy: 0.2708"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 13:56:49.361260: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_381788\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:199\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_4_ResNet152Transformer_set3_D2.hdf5\n",
      "1203/1203 [==============================] - 1012s 841ms/step - loss: 3.6948 - accuracy: 0.2708 - val_loss: 3.1891 - val_accuracy: 0.4716 - lr: 1.0332e-04\n",
      "0\n",
      "flg_i 0 flag 0\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_26977/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 29220.32758555424), (1, 28735.421124690667), (2, 28965.997785978892), (3, 0)]\n",
      "distances  [(3, 0), (1, 28735.421124690667), (2, 28965.997785978892), (0, 29220.32758555424)]\n",
      "neighbors ids  [3, 1, 2, 0]\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.471590906381607\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.7968073487281799\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.6120129823684692\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.761904776096344\n",
      "neighbor best  [(1, 0.7968073487281799), (0, 0.761904776096344), (2, 0.6120129823684692), (3, 0.471590906381607)]\n",
      "name_file_neighbor_best  ucf101_2_ResNet152Transformer_set3_D2_best.hdf5\n",
      "u  0.2\n",
      "distance_ij  28735.421124690667\n",
      "tmp_lr  0.00010331847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/4153759187.py:237: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  - M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j)) \\\n",
      "/tmp/ipykernel_26977/4153759187.py:238: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  + M*u*tmp_lr/(1+distance_ij)*np.array(particle_w_j)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  0.2\n",
      "distance_ij  28965.997785978892\n",
      "tmp_lr  0.00010331847\n",
      "u  0.2\n",
      "distance_ij  29220.32758555424\n",
      "tmp_lr  0.00010331847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/4153759187.py:251: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 4  r1= 0.5089442359000053  r2= 0.8623414349068029  current acc= 0.47159090638160706  local best= 0.47159090638160706  neighbor index= 1  neighbor best= 0.7968073487281799\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 14:10:31.536235: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_442187\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:221\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  0.011557661442155018\n",
      "1203/1203 [==============================] - ETA: 0s - loss: 0.9274 - accuracy: 0.7449"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 14:24:45.267008: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_448398\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:243\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_4_ResNet152Transformer_set3_D2.hdf5\n",
      "1203/1203 [==============================] - 1031s 857ms/step - loss: 0.9274 - accuracy: 0.7449 - val_loss: 1.6462 - val_accuracy: 0.6115 - lr: 0.0116\n",
      "0\n",
      "flg_i 0 flag 1\n",
      "flg_i 1 flag 1\n",
      "flg_i 2 flag 1\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_26977/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 25396.591609672254), (1, 25863.368824680532), (2, 25899.37284866949), (3, 0)]\n",
      "distances  [(3, 0), (0, 25396.591609672254), (1, 25863.368824680532), (2, 25899.37284866949)]\n",
      "neighbors ids  [3, 0, 1, 2]\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.6114718317985535\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.761904776096344\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.8084415793418884\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.7080627679824829\n",
      "neighbor best  [(1, 0.8084415793418884), (0, 0.761904776096344), (2, 0.7080627679824829), (3, 0.6114718317985535)]\n",
      "name_file_neighbor_best  ucf101_2_ResNet152Transformer_set3_D2_best.hdf5\n",
      "u  0.2\n",
      "distance_ij  25396.591609672254\n",
      "tmp_lr  0.011557661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/4153759187.py:237: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  - M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j)) \\\n",
      "/tmp/ipykernel_26977/4153759187.py:238: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  + M*u*tmp_lr/(1+distance_ij)*np.array(particle_w_j)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  0.2\n",
      "distance_ij  25863.368824680532\n",
      "tmp_lr  0.011557661\n",
      "u  0.2\n",
      "distance_ij  25899.37284866949\n",
      "tmp_lr  0.011557661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/4153759187.py:251: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 5  r1= 0.3076292654412093  r2= 0.030033295313204977  current acc= 0.6114718317985535  local best= 0.6114718317985535  neighbor index= 1  neighbor best= 0.8084415793418884\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 14:39:37.101820: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_508797\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:265\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  0.02150591622368088\n",
      "1203/1203 [==============================] - ETA: 0s - loss: 1.2040 - accuracy: 0.6903"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 14:53:45.678674: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_515008\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:287\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_4_ResNet152Transformer_set3_D2.hdf5\n",
      "1203/1203 [==============================] - 1027s 853ms/step - loss: 1.2040 - accuracy: 0.6903 - val_loss: 2.1291 - val_accuracy: 0.5411 - lr: 0.0215\n",
      "0\n",
      "flg_i 0 flag 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 1\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_26977/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 52941.954522473876), (1, 63858.18961028925), (2, 62579.89229816292), (3, 0)]\n",
      "distances  [(3, 0), (0, 52941.954522473876), (2, 62579.89229816292), (1, 63858.18961028925)]\n",
      "neighbors ids  [3, 0, 2, 1]\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.6114718317985535\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.761904776096344\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.7713744640350342\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.8084415793418884\n",
      "neighbor best  [(1, 0.8084415793418884), (2, 0.7713744640350342), (0, 0.761904776096344), (3, 0.6114718317985535)]\n",
      "name_file_neighbor_best  ucf101_2_ResNet152Transformer_set3_D2_best.hdf5\n",
      "u  0.2\n",
      "distance_ij  52941.954522473876\n",
      "tmp_lr  0.021505916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/4153759187.py:237: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  - M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j)) \\\n",
      "/tmp/ipykernel_26977/4153759187.py:238: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  + M*u*tmp_lr/(1+distance_ij)*np.array(particle_w_j)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  0.2\n",
      "distance_ij  62579.89229816292\n",
      "tmp_lr  0.021505916\n",
      "u  0.2\n",
      "distance_ij  63858.18961028925\n",
      "tmp_lr  0.021505916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/4153759187.py:251: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 6  r1= 0.8563289820329761  r2= 0.6397293574033821  current acc= 0.5411255359649658  local best= 0.6114718317985535  neighbor index= 1  neighbor best= 0.8084415793418884\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 15:08:40.515788: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_571934\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:309\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  0.001414045824687533\n",
      "1203/1203 [==============================] - ETA: 0s - loss: 0.1615 - accuracy: 0.9541"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 15:22:53.108771: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_578145\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:331\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_4_ResNet152Transformer_set3_D2.hdf5\n",
      "1203/1203 [==============================] - 1041s 864ms/step - loss: 0.1615 - accuracy: 0.9541 - val_loss: 0.8889 - val_accuracy: 0.8003 - lr: 0.0014\n",
      "0\n",
      "flg_i 0 flag 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 1\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_26977/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 32532.08166605144), (1, 47057.82446193022), (2, 51052.0967634098), (3, 0)]\n",
      "distances  [(3, 0), (0, 32532.08166605144), (1, 47057.82446193022), (2, 51052.0967634098)]\n",
      "neighbors ids  [3, 0, 1, 2]\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.8003246784210205\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.761904776096344\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.8195346593856812\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.7997835278511047\n",
      "neighbor best  [(1, 0.8195346593856812), (3, 0.8003246784210205), (2, 0.7997835278511047), (0, 0.761904776096344)]\n",
      "name_file_neighbor_best  ucf101_2_ResNet152Transformer_set3_D2_best.hdf5\n",
      "u  0.2\n",
      "distance_ij  32532.08166605144\n",
      "tmp_lr  0.0014140458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/4153759187.py:237: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  - M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j)) \\\n",
      "/tmp/ipykernel_26977/4153759187.py:238: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  + M*u*tmp_lr/(1+distance_ij)*np.array(particle_w_j)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  0.2\n",
      "distance_ij  47057.82446193022\n",
      "tmp_lr  0.0014140458\n",
      "u  0.2\n",
      "distance_ij  51052.0967634098\n",
      "tmp_lr  0.0014140458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/4153759187.py:251: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 7  r1= 0.4858655720970937  r2= 0.14178384406448197  current acc= 0.8003246784210205  local best= 0.8003246784210205  neighbor index= 1  neighbor best= 0.8195346593856812\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 15:38:03.757733: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_638544\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:353\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  7.659762614243144e-05\n",
      "1203/1203 [==============================] - ETA: 0s - loss: 0.0896 - accuracy: 0.9748"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 15:52:17.604980: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_644755\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:375\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_4_ResNet152Transformer_set3_D2.hdf5\n",
      "1203/1203 [==============================] - 1021s 848ms/step - loss: 0.0896 - accuracy: 0.9748 - val_loss: 0.8285 - val_accuracy: 0.8079 - lr: 7.6598e-05\n",
      "0\n",
      "flg_i 0 flag 0\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_26977/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 36678.15683891219), (1, 38152.08381334135), (2, 40905.038517554036), (3, 0)]\n",
      "distances  [(3, 0), (0, 36678.15683891219), (1, 38152.08381334135), (2, 40905.038517554036)]\n",
      "neighbors ids  [3, 0, 1, 2]\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.8079004287719727\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.761904776096344\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.8195346593856812\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.8170995712280273\n",
      "neighbor best  [(1, 0.8195346593856812), (2, 0.8170995712280273), (3, 0.8079004287719727), (0, 0.761904776096344)]\n",
      "name_file_neighbor_best  ucf101_2_ResNet152Transformer_set3_D2_best.hdf5\n",
      "u  0.2\n",
      "distance_ij  36678.15683891219\n",
      "tmp_lr  7.659763e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/4153759187.py:237: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  - M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j)) \\\n",
      "/tmp/ipykernel_26977/4153759187.py:238: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  + M*u*tmp_lr/(1+distance_ij)*np.array(particle_w_j)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  0.2\n",
      "distance_ij  38152.08381334135\n",
      "tmp_lr  7.659763e-05\n",
      "u  0.2\n",
      "distance_ij  40905.038517554036\n",
      "tmp_lr  7.659763e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/4153759187.py:251: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 8  r1= 0.9492641864640584  r2= 0.21312671022280227  current acc= 0.8079004287719727  local best= 0.8079004287719727  neighbor index= 1  neighbor best= 0.8195346593856812\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 16:05:52.264961: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_705154\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:397\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  0.000518803870002762\n",
      "1203/1203 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9789"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 16:19:55.673354: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_711365\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:419\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_4_ResNet152Transformer_set3_D2.hdf5\n",
      "1203/1203 [==============================] - 1024s 851ms/step - loss: 0.0732 - accuracy: 0.9789 - val_loss: 0.7820 - val_accuracy: 0.8193 - lr: 5.1880e-04\n",
      "0\n",
      "flg_i 0 flag 1\n",
      "flg_i 1 flag 1\n",
      "flg_i 2 flag 1\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 1\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_26977/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 23433.205419302656), (1, 33228.26902844394), (2, 35504.613472725854), (3, 0)]\n",
      "distances  [(3, 0), (0, 23433.205419302656), (1, 33228.26902844394), (2, 35504.613472725854)]\n",
      "neighbors ids  [3, 0, 1, 2]\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.8192640542984009\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.761904776096344\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.822510838508606\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.8184523582458496\n",
      "neighbor best  [(1, 0.822510838508606), (3, 0.8192640542984009), (2, 0.8184523582458496), (0, 0.761904776096344)]\n",
      "name_file_neighbor_best  ucf101_2_ResNet152Transformer_set3_D2_best.hdf5\n",
      "u  0.2\n",
      "distance_ij  23433.205419302656\n",
      "tmp_lr  0.0005188039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/4153759187.py:237: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  - M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j)) \\\n",
      "/tmp/ipykernel_26977/4153759187.py:238: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  + M*u*tmp_lr/(1+distance_ij)*np.array(particle_w_j)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  0.2\n",
      "distance_ij  33228.26902844394\n",
      "tmp_lr  0.0005188039\n",
      "u  0.2\n",
      "distance_ij  35504.613472725854\n",
      "tmp_lr  0.0005188039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/4153759187.py:251: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 9  r1= 0.8269959193985306  r2= 0.20249534041975037  current acc= 0.8192640542984009  local best= 0.8192640542984009  neighbor index= 1  neighbor best= 0.822510838508606\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 16:36:00.114720: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_771764\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:441\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random lr =  0.055829937350039986\n",
      "1203/1203 [==============================] - ETA: 0s - loss: 4.1278 - accuracy: 0.1495"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 16:49:55.651554: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_777975\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:463\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_4_ResNet152Transformer_set3_D2.hdf5\n",
      "1203/1203 [==============================] - 998s 829ms/step - loss: 4.1278 - accuracy: 0.1495 - val_loss: 3.9713 - val_accuracy: 0.1615 - lr: 0.0558\n",
      "0\n",
      "flg_i 0 flag 0\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 1\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_26977/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1203/1203 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9866"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 21:10:18.837207: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_1349681\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:859\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to checkpoints/ucf101_4_ResNet152Transformer_set3_D2.hdf5\n",
      "1203/1203 [==============================] - 1050s 872ms/step - loss: 0.0505 - accuracy: 0.9866 - val_loss: 0.7035 - val_accuracy: 0.8360 - lr: 3.6690e-05\n",
      "0\n",
      "flg_i 0 flag 1\n",
      "flg_i 1 flag 1\n",
      "flg_i 2 flag 1\n",
      "flg_i 3 flag 0\n",
      "\n",
      "\n",
      "flg_i 0 flag 0.. 1\n",
      "flg_i 1 flag 0\n",
      "flg_i 2 flag 0\n",
      "flg_i 3 flag 0\n",
      "end of waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/1838896777.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_1 = np.array(w1)\n",
      "/tmp/ipykernel_26977/1838896777.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  w_np_2 = np.array(w2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances unsorted [(0, 24818.6825419009), (1, 24137.648037232444), (2, 16626.561424644817), (3, 0)]\n",
      "distances  [(3, 0), (2, 16626.561424644817), (1, 24137.648037232444), (0, 24818.6825419009)]\n",
      "neighbors ids  [3, 2, 1, 0]\n",
      "neighbor_idx  3\n",
      "neighbor_best_tmp  0.8384740352630615\n",
      "neighbor_idx  2\n",
      "neighbor_best_tmp  0.8463203310966492\n",
      "neighbor_idx  1\n",
      "neighbor_best_tmp  0.834956705570221\n",
      "neighbor_idx  0\n",
      "neighbor_best_tmp  0.800054132938385\n",
      "neighbor best  [(2, 0.8463203310966492), (3, 0.8384740352630615), (1, 0.834956705570221), (0, 0.800054132938385)]\n",
      "name_file_neighbor_best  ucf101_3_ResNet152Transformer_set3_D2_best.hdf5\n",
      "u  0.2\n",
      "distance_ij  16626.561424644817\n",
      "tmp_lr  3.6690202e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/4153759187.py:237: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  - M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j)) \\\n",
      "/tmp/ipykernel_26977/4153759187.py:238: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  + M*u*tmp_lr/(1+distance_ij)*np.array(particle_w_j)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u  0.2\n",
      "distance_ij  24137.648037232444\n",
      "tmp_lr  3.6690202e-05\n",
      "u  0.2\n",
      "distance_ij  24818.6825419009\n",
      "tmp_lr  3.6690202e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26977/4153759187.py:251: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ---> epoch= 19  r1= 0.43924882913582564  r2= 0.5977651390006096  current acc= 0.8360389471054077  local best= 0.8384740352630615  neighbor index= 2  neighbor best= 0.8463203310966492\n",
      "[0.01163419894874096, 0.01163419894874096, 0.027326839044690132, 0.27218616008758545, 0.47159090638160706, 0.6114718317985535, 0.5411255359649658, 0.8003246784210205, 0.8079004287719727, 0.8192640542984009, 0.16152597963809967, 0.29843074083328247, 0.2954545319080353, 0.6585497856140137, 0.698051929473877, 0.8062770366668701, 0.8152056336402893, 0.8384740352630615, 0.8357684016227722, 0.8360389471054077]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import keras\n",
    "import math\n",
    "\n",
    "#index of this pso\n",
    "pso_index = 3\n",
    "\n",
    "#number of neighbors (max=4)\n",
    "num_neighbors = 4\n",
    "#K coefficient\n",
    "M = 1\n",
    "u = 1\n",
    "\n",
    "tmp_acc = 0\n",
    "tmp_w = []\n",
    "pbest_acc = 0\n",
    "pbest_w = []\n",
    "\n",
    "#accelerator coefficient\n",
    "c1 = 0.5\n",
    "c2 = 0.5\n",
    "# w = 0.5\n",
    "\n",
    "r1 = 0\n",
    "r2 = 0\n",
    "\n",
    "results_stack_accuracy = []\n",
    "results_stack_val_accuracy = []\n",
    "results_stack_loss = []\n",
    "results_stack_val_loss = []\n",
    "\n",
    "#threshold\n",
    "# threshold = 0.97\n",
    "\n",
    "# #iteration control\n",
    "# i = 0\n",
    "# iter_max = 40\n",
    "\n",
    "warm_up = 0\n",
    "\n",
    "#    \n",
    "# time synchronize\n",
    "number_of_pso = 4\n",
    "training_start_flag = 1\n",
    "training_finish_flag = 0\n",
    "\n",
    "#set initial training flag to start\n",
    "set_training_flag(pso_index, training_start_flag)\n",
    "\n",
    "for index in range(warm_up, epochs): \n",
    "# while i < iter_max:\n",
    "    #start training \n",
    "    set_training_flag(pso_index, training_start_flag)\n",
    "    print(get_training_flag(pso_index))\n",
    "    \n",
    "    #save previous weight\n",
    "    model_mul.save_weights(savedfilename_pre) \n",
    "    \n",
    "    # result = model_mul.fit_generator(\n",
    "    #     generator = train_set, \n",
    "    #     steps_per_epoch = step_size_train,\n",
    "    #     validation_data = valid_set,\n",
    "    #     validation_steps = step_size_valid,\n",
    "    #     shuffle=True,\n",
    "    #     epochs=1,\n",
    "    #     callbacks=[checkpointer,tf.keras.callbacks.LearningRateScheduler(rand_scheduler)],\n",
    "    # #     callbacks=[csv_logger, checkpointer, earlystopping],\n",
    "    # #     callbacks=[tb, csv_logger, checkpointer, earlystopping],        \n",
    "    #     verbose=1) \n",
    "\n",
    "    result = model_mul.fit(\n",
    "        train,\n",
    "        validation_data=test,\n",
    "        verbose=1,   \n",
    "        epochs=1,\n",
    "        callbacks=[checkpointer,tf.keras.callbacks.LearningRateScheduler(rand_scheduler)],\n",
    "        # workers=2\n",
    "    ) \n",
    "    \n",
    "    #save weights every iteration\n",
    "#     model_mul.save_weights(savedfilename)\n",
    "    \n",
    "    tmp_acc = result.history.get('val_accuracy')[-1]\n",
    "    tmp_w = model_mul.get_weights()\n",
    "    tmp_lr = result.history.get('lr')[-1]\n",
    "    \n",
    "    #save current location in scoreboard\n",
    "    set_c_loc(pso_index,tmp_acc) \n",
    "    \n",
    "    if tmp_acc > pbest_acc:\n",
    "        pbest_acc = tmp_acc\n",
    "        pbest_w = tmp_w\n",
    "        #save person best location\n",
    "        set_pbest_loc(pso_index,pbest_acc)  \n",
    "        # save best model\n",
    "        model_mul.save_weights(savedfilename_best)        \n",
    "\n",
    "    #set training flag to finish\n",
    "    set_training_flag(pso_index, training_finish_flag)  \n",
    "    print(get_training_flag(pso_index))\n",
    "        \n",
    "    # check if all PSOs is ready (flag==1)\n",
    "    while(True):\n",
    "        tmp_flag = 0\n",
    "        for flg_i in range(number_of_pso):\n",
    "            print(\"flg_i\", flg_i, \"flag\", get_training_flag(flg_i))\n",
    "            if(get_training_flag(flg_i) == 1):\n",
    "                tmp_flag = 1\n",
    "        if(tmp_flag==1):\n",
    "            #waiting for 60s\n",
    "            print(\"\\n\")\n",
    "            for i in range(60,0,-1):\n",
    "                print(\"waiting for ....%2d\" %i, end=\"\\r\", flush=True)\n",
    "                time.sleep(1)    \n",
    "        else:\n",
    "            print(\"end of waiting\")\n",
    "            break  \n",
    "    \n",
    "    r1 = random.uniform(0,1)\n",
    "    r2 = random.uniform(0,1)\n",
    "#     r3 = random.uniform(0,1)    \n",
    "    \n",
    "    #-----------nearest neighbor best--------------\n",
    "    #get neighbor weights\n",
    "    #1\n",
    "    neighbor_c_acc_1, name_file_1 = get_c_loc(0)\n",
    "    neighbor_c_acc_2, name_file_2 = get_c_loc(1)\n",
    "    neighbor_c_acc_3, name_file_3 = get_c_loc(2)\n",
    "    neighbor_c_acc_4, name_file_4 = get_c_loc(3)\n",
    "    \n",
    "    #get pre loc\n",
    "    neighbor_pre_acc_1, name_pre_file_1 = get_pre_loc(0)\n",
    "    neighbor_pre_acc_2, name_pre_file_2 = get_pre_loc(1)\n",
    "    neighbor_pre_acc_3, name_pre_file_3 = get_pre_loc(2)\n",
    "    neighbor_pre_acc_4, name_pre_file_4 = get_pre_loc(3)  \n",
    "    \n",
    "    #clone model for weights change\n",
    "    model_clone = keras.models.clone_model(model_mul)\n",
    "    \n",
    "    model_clone.load_weights(os.path.join('checkpoints', name_file_1))\n",
    "    neighbor_w_1 = model_clone.get_weights() \n",
    "    model_clone.load_weights(os.path.join('checkpoints', name_file_2))\n",
    "    neighbor_w_2 = model_clone.get_weights()\n",
    "    model_clone.load_weights(os.path.join('checkpoints', name_file_3))\n",
    "    neighbor_w_3 = model_clone.get_weights()\n",
    "    model_clone.load_weights(os.path.join('checkpoints', name_file_4))\n",
    "    neighbor_w_4 = model_clone.get_weights()\n",
    "    \n",
    "    #clone model pre weights\n",
    "    model_clone.load_weights(os.path.join('checkpoints', name_pre_file_1))\n",
    "    neighbor_pre_w_1 = model_clone.get_weights()     \n",
    "    model_clone.load_weights(os.path.join('checkpoints', name_pre_file_2))\n",
    "    neighbor_pre_w_2 = model_clone.get_weights() \n",
    "    model_clone.load_weights(os.path.join('checkpoints', name_pre_file_3))\n",
    "    neighbor_pre_w_3 = model_clone.get_weights()    \n",
    "    model_clone.load_weights(os.path.join('checkpoints', name_pre_file_4))\n",
    "    neighbor_pre_w_4 = model_clone.get_weights()  \n",
    "    \n",
    "    distance_1 = find_distance(neighbor_w_4,neighbor_w_1)\n",
    "    distance_2 = find_distance(neighbor_w_4,neighbor_w_2)\n",
    "    distance_3 = find_distance(neighbor_w_4,neighbor_w_3)\n",
    "    \n",
    "    #find the closest neighbor\n",
    "    distances = list()\n",
    "    distances.append((0,distance_1))\n",
    "    distances.append((1,distance_2))\n",
    "    distances.append((2,distance_3))\n",
    "    distances.append((3,0))\n",
    "\n",
    "    print('distances unsorted', distances)\n",
    "    \n",
    "    distances.sort(key=lambda tup: tup[1])\n",
    "    print('distances ', distances)\n",
    "    \n",
    "    neighbors_idx = list()\n",
    "    for i in range(num_neighbors):\n",
    "        neighbors_idx.append(distances[i][0])        \n",
    "    \n",
    "    print('neighbors ids ', neighbors_idx)\n",
    "    \n",
    "    #get neighbor bests from the list\n",
    "    neighbor_bests = list()\n",
    "    #remove first element (self distance)\n",
    "#     neighbors_idx.pop(0)\n",
    "    \n",
    "    for i in range(len(neighbors_idx)):\n",
    "        neighbor_best_tmp, name_file_neighbor_best_tmp = get_pbest_loc(neighbors_idx[i])\n",
    "        neighbor_bests.append((neighbors_idx[i],neighbor_best_tmp))\n",
    "        print('neighbor_idx ', neighbors_idx[i])\n",
    "        print('neighbor_best_tmp ', neighbor_best_tmp)\n",
    "\n",
    "    # keep unsorted list of neighbor\n",
    "    neighbor_tmp = deepcopy(neighbor_bests)       \n",
    "    \n",
    "    # sort the list for maximum accuracy   \n",
    "    neighbor_bests.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    print('neighbor best ', neighbor_bests)\n",
    "    #\n",
    "    neighbor_best_value, name_file_neighbor_best = get_pbest_loc(neighbor_bests[0][0])\n",
    "    print('name_file_neighbor_best ', name_file_neighbor_best)\n",
    "    \n",
    "    model_clone.load_weights(os.path.join('checkpoints', name_file_neighbor_best))\n",
    "    neighbor_best_w = model_clone.get_weights()  \n",
    "    #---------- end nearest neighbor best ----------\n",
    "    \n",
    "    #---------- cucker -----------------------------\n",
    "    particle_w_i = neighbor_w_4\n",
    "    sum_particle_tmp = 0\n",
    "    \n",
    "    #remove the fist (self)\n",
    "    neighbor_tmp.pop(0)\n",
    "    \n",
    "    for j in range(len(neighbor_tmp)):\n",
    "        if neighbor_tmp[j][0]==0:\n",
    "            particle_w_j = neighbor_w_1\n",
    "            particle_w_pre_j = neighbor_pre_w_1\n",
    "            distance_ij = distance_1\n",
    "            u = 0.2\n",
    "        elif neighbor_tmp[j][0]==1:    \n",
    "            particle_w_j = neighbor_w_2\n",
    "            particle_w_pre_j = neighbor_pre_w_2\n",
    "            distance_ij = distance_2\n",
    "            u = 0.2\n",
    "        elif neighbor_tmp[j][0]==2:    \n",
    "            particle_w_j = neighbor_w_3\n",
    "            particle_w_pre_j = neighbor_pre_w_3\n",
    "            distance_ij = distance_3\n",
    "            u = 0.2\n",
    "            \n",
    "        print('u ', u)\n",
    "        print('distance_ij ', distance_ij)\n",
    "        print('tmp_lr ', tmp_lr)\n",
    "        #sum(K/(1+distance)*(particle_w_j-particle_w_i)\n",
    "#         sum_particle_tmp -= M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j))\n",
    "        sum_particle_tmp =  sum_particle_tmp \\\n",
    "                            - M*u/(1+distance_ij)*(np.array(particle_w_pre_j)-np.array(particle_w_j)) \\\n",
    "                            + M*u*tmp_lr/(1+distance_ij)*np.array(particle_w_j)\n",
    "        \n",
    "    #---------- end cucker -------------------------\n",
    "\n",
    "    #update networks' weights\n",
    "    #     w = c1*r1*(np.array(pbest_w)-np.array(tmp_w))+c2*r2*(np.array(gbest_w)-np.array(tmp_w))\n",
    "    #     w = r1*np.array(pbest_w)+r2*np.array(tmp_w)+r3*np.array(gbest_w)\n",
    "    #     w = np.array(tmp_w)+tmp_lr*(c1*r1*(np.array(pbest_w)-np.array(tmp_w))+c2*r2*(np.array(gbest_w)-np.array(tmp_w)))\n",
    "#     final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n",
    "#     final_weight = np.array(tmp_w)+sum_particle_tmp\n",
    "\n",
    "#     final_weight = np.array(tmp_w)+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n",
    "#     final_weight = np.array(tmp_w)+sum_particle_tmp+c1*r1*(np.array(pbest_w)-np.array(tmp_w))+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n",
    "    final_weight = np.array(tmp_w)+sum_particle_tmp+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n",
    "#     final_weight = np.array(tmp_w)+c2*r2*(np.array(neighbor_best_w)-np.array(tmp_w))\n",
    "#     final_weight = np.array(tmp_w)+sum_particle_tmp\n",
    "    \n",
    "    model_mul.set_weights(final_weight)\n",
    "    \n",
    "    print('After ---> epoch=', index, ' r1=',r1, ' r2=',r2, ' current acc=', tmp_acc, ' local best=', pbest_acc, \n",
    "          ' neighbor index=', neighbor_bests[0][0], ' neighbor best=', neighbor_best_value)  \n",
    "    \n",
    "    results_stack_val_accuracy.append(result.history.get('val_accuracy')[-1])\n",
    "    results_stack_accuracy.append(result.history.get('accuracy')[-1])\n",
    "    results_stack_val_loss.append(result.history.get('val_loss')[-1])      \n",
    "    results_stack_loss.append(result.history.get('loss')[-1])\n",
    "    \n",
    "#     i = i + 1\n",
    "        \n",
    "print(results_stack_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73b14400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.8201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "760b96d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8384740352630615\n"
     ]
    }
   ],
   "source": [
    "print(max(results_stack_val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae78b79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e14f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !wget --no-check-certificate https://www.crcv.ucf.edu/data/UCF101/UCF101.rar\n",
    "# !wget --no-check-certificate https://www.crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05772bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !unrar e UCF101.rar data/\n",
    "# !unzip -qq UCF101TrainTestSplits-RecognitionTask.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb246c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move_videos(train_new, \"train\")\n",
    "# move_videos(test_new, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140807dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec4358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %mv UCF101.rar ucf101_v2\n",
    "# %mv UCF101TrainTestSplits-RecognitionTask.zip ucf101_v2\n",
    "# %mkdir ucf101_v2/set1\n",
    "# %mv train ucf101_v2/set1\n",
    "# %mv test ucf101_v2/set1\n",
    "# %mv data ucf101_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cdee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222c8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f32ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
